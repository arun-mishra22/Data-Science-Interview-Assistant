{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1373027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca \n",
    "# OpenAssistant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb381e5",
   "metadata": {},
   "source": [
    "#### Loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "969c403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/arun4/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f4300c3b8443afbe32805258bfcd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c471e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caba818",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"data science\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural network\",\n",
    "    \"statistics\",\n",
    "    \"probability\",\n",
    "    \"python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"sql\",\n",
    "    \"nlp\",\n",
    "    \"computer vision\",\n",
    "    \"regression\",\n",
    "    \"classification\",\n",
    "    \"clustering\",\n",
    "    \"model\",\n",
    "    \"overfitting\",\n",
    "    \"underfitting\",\n",
    "    \"gradient descent\",\n",
    "    \"interview\",\n",
    "    \"data analysis\",\n",
    "    \"feature engineering\",\n",
    "    \"data preprocessing\",\n",
    "    \"data visualization\",\n",
    "    \"decision tree\",\n",
    "    \"random forest\",\n",
    "    \"xgboost\",\n",
    "    \"transformer\",\n",
    "    \"llm\",\n",
    "    \"mlops\",\n",
    "    \"hyperparameter\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ds_related(example):\n",
    "    instruction = example[\"instruction\"].lower()\n",
    "    \n",
    "    return any(keyword in instruction for keyword in keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33219de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6416cb98a93f48228efc10589c3fb982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "\n",
    "filtered_data = train_data.filter(filter_ds_related)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d37887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 52002\n",
      "Filtered size: 2226\n"
     ]
    }
   ],
   "source": [
    "print(\"Original size:\", len(train_data))\n",
    "print(\"Filtered size:\", len(filtered_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbdb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 2226\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93780007",
   "metadata": {},
   "source": [
    "##### cleaning the filtered data set  -->\n",
    "Remove noisy records\n",
    "\n",
    "Fix formatting issues\n",
    "\n",
    "Remove very short/very long samples\n",
    "\n",
    "Remove bad quality answers\n",
    "\n",
    "Standardize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def basic_clean(example):\n",
    "    def clean_text(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    return {\n",
    "        \"instruction\": clean_text(example[\"instruction\"]),\n",
    "        \"input\": clean_text(example[\"input\"]) if example[\"input\"] else \"\",\n",
    "        \"output\": clean_text(example[\"output\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2bbe489341402e942150e2dfc47491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned = filtered_data.map(basic_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afcf38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ba801576a849bd942aa0e35568c1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Remove Low Quality Records\n",
    "\n",
    "def remove_empty(example):\n",
    "    return len(example[\"instruction\"]) > 10 and len(example[\"output\"]) > 10\n",
    "\n",
    "cleaned = cleaned.filter(remove_empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Very Short or Very Long Samples\n",
    "\n",
    "def length_filter(example):\n",
    "    total_len = len(example[\"instruction\"]) + len(example[\"output\"])\n",
    "    return 30 < total_len < 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae70be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485a295a0a004c3ea36cd9cceb60381a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned = cleaned.filter(length_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af32ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 2190\n"
     ]
    }
   ],
   "source": [
    "## Remove Duplicates\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to pandas\n",
    "df = cleaned.to_pandas()\n",
    "\n",
    "# Remove duplicate rows based on instruction + output\n",
    "df = df.drop_duplicates(subset=[\"instruction\", \"output\"])\n",
    "\n",
    "# Convert back to HuggingFace dataset\n",
    "cleaned_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(\"After deduplication:\", len(cleaned_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f20031e1de4df49eb3f1a60953bd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create Final Prompt Format\n",
    "\n",
    "def create_prompt(example):\n",
    "    if example[\"input\"]:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{example[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\"\"\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "final_cleaned = cleaned.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05979378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned size: 2190\n",
      "{'instruction': 'Render a 3D model of a house', 'input': '', 'output': '<nooutput> This type of instruction cannot be fulfilled by a GPT model.', 'text': '### Instruction:\\nRender a 3D model of a house\\n\\n### Response:\\n<nooutput> This type of instruction cannot be fulfilled by a GPT model.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned size:\", len(final_cleaned))\n",
    "print(final_cleaned[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Non-Textual / Non-Answerable Tasks\n",
    "\n",
    "bad_keywords = [\n",
    "    \"render\",\n",
    "    \"draw\",\n",
    "    \"paint\",\n",
    "    \"image of\",\n",
    "    \"picture of\",\n",
    "    \"3d model\",\n",
    "    \"generate an image\",\n",
    "    \"create a video\",\n",
    "    \"audio file\",\n",
    "    \"physical\",\n",
    "    \"real world action\"\n",
    "]\n",
    "\n",
    "def remove_bad_tasks(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return not any(k in text for k in bad_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d81e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa1bed163f4fee8048add97d5ff4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing non-text tasks: 2179\n"
     ]
    }
   ],
   "source": [
    "cleaned = cleaned.filter(remove_bad_tasks)\n",
    "\n",
    "print(\"After removing non-text tasks:\", len(cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d2d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8333ac8b313447418a407fb6aec1d276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"alpaca_ds_fully_cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a486b5b",
   "metadata": {},
   "source": [
    "Loading open-assistant dataset -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51a219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/arun4/.cache/huggingface/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23dc779f0ee45ffa273729667a11e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "        num_rows: 84437\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "        num_rows: 4401\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load OpenAssistant dataset\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213afc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'parent_id': None, 'user_id': 'c3fe8c76-fc30-4fa7-b7f8-c492f5967d18', 'created_date': '2023-02-05T14:23:50.983374+00:00', 'text': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.', 'role': 'prompter', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': None, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.00044308538781479, 'severe_toxicity': 3.252684837207198e-05, 'obscene': 0.00023475120542570949, 'identity_attack': 0.0001416115992469713, 'insult': 0.00039489680784754455, 'threat': 4.075629112776369e-05, 'sexual_explicit': 2.712695459194947e-05}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': {'name': ['+1', '_skip_reply', '_skip_ranking'], 'count': [10, 1, 4]}, 'labels': {'name': ['spam', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9166666666666666, 0.16666666666666666, 0.3333333333333333, 0.6666666666666666, 0.0], 'count': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}}\n",
      "{'message_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c', 'parent_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'user_id': '2c96e467-66f0-4be7-9693-bda51356a424', 'created_date': '2023-02-06T13:50:44.657083+00:00', 'text': '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.', 'role': 'assistant', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': 0, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.00026396565954200923, 'severe_toxicity': 2.7454958399175666e-05, 'obscene': 0.00013584605767391622, 'identity_attack': 9.263094398193061e-05, 'insult': 0.0001668655313551426, 'threat': 3.769186878344044e-05, 'sexual_explicit': 2.500762275303714e-05}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': {'name': ['+1', '_skip_labeling'], 'count': [3, 1]}, 'labels': {'name': ['spam', 'fails_task', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'helpfulness', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9166666666666666, 0.375, 0.375, 0.75, 0.375, 0.0], 'count': [3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2]}}\n",
      "{'message_id': '6708c47f-05c9-4346-b3d2-40b2bd24fde4', 'parent_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c', 'user_id': '2c96e467-66f0-4be7-9693-bda51356a424', 'created_date': '2023-02-06T18:48:49.391686+00:00', 'text': 'Now explain it to a dog', 'role': 'prompter', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': None, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.03648477792739868, 'severe_toxicity': 5.486844383995049e-05, 'obscene': 0.0003762090636882931, 'identity_attack': 0.0002415566414128989, 'insult': 0.013612336479127407, 'threat': 0.0017075861105695367, 'sexual_explicit': 0.00010235361696686596}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': None, 'labels': {'name': ['spam', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.5, 0.625, 0.5, 0.0], 'count': [3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2]}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"train\"][1])\n",
    "print(dataset[\"train\"][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bd924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompter', 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "roles = set(dataset[\"train\"][\"role\"])\n",
    "print(roles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7b55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 prompter\n",
      "1 assistant\n",
      "2 prompter\n",
      "3 assistant\n",
      "4 prompter\n",
      "5 assistant\n",
      "6 prompter\n",
      "7 assistant\n",
      "8 assistant\n",
      "9 assistant\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, dataset[\"train\"][i][\"role\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd55a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[\"train\"]  \n",
    "alpaca_style = []\n",
    "\n",
    "for i in range(len(data) - 1):\n",
    "    current = data[i]\n",
    "    next_msg = data[i + 1]\n",
    "\n",
    "    # Only take valid prompter â†’ assistant pairs\n",
    "    if current[\"role\"] == \"prompter\" and next_msg[\"role\"] == \"assistant\":\n",
    "        alpaca_style.append({\n",
    "            \"instruction\": current[\"text\"].strip(),\n",
    "            \"input\": \"\",\n",
    "            \"output\": next_msg[\"text\"].strip()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372e1f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 27904\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# convert to hugging face dataset \n",
    "from datasets import Dataset\n",
    "\n",
    "oa_dataset = Dataset.from_list(alpaca_style)\n",
    "\n",
    "print(oa_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3584f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"data science\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural network\",\n",
    "    \"statistics\",\n",
    "    \"probability\",\n",
    "    \"python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"sql\",\n",
    "    \"nlp\",\n",
    "    \"computer vision\",\n",
    "    \"regression\",\n",
    "    \"classification\",\n",
    "    \"clustering\",\n",
    "    \"model\",\n",
    "    \"overfitting\",\n",
    "    \"underfitting\",\n",
    "    \"gradient descent\",\n",
    "    \"interview\",\n",
    "    \"data analysis\",\n",
    "    \"feature engineering\",\n",
    "    \"data preprocessing\",\n",
    "    \"data visualization\",\n",
    "    \"decision tree\",\n",
    "    \"random forest\",\n",
    "    \"xgboost\",\n",
    "    \"transformer\",\n",
    "    \"llm\",\n",
    "    \"mlops\",\n",
    "    \"hyperparameter\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7a3ec",
   "metadata": {},
   "source": [
    "filtering rows for data science domain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4598ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f8c6889e114b99805749a0bedc7f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after DS filtering: 1165\n"
     ]
    }
   ],
   "source": [
    "def filter_ds_related(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "oa_filtered = oa_dataset.filter(filter_ds_related)\n",
    "\n",
    "print(\"Rows after DS filtering:\", len(oa_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b00549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adb921e3afb4f39a8ae3144056b800c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## basic cleaning - remove extra space and normalize text \n",
    "\n",
    "import re\n",
    "\n",
    "def basic_clean(example):\n",
    "    def normalize(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # remove multiple spaces\n",
    "        return text\n",
    "\n",
    "    return {\n",
    "        \"instruction\": normalize(example[\"instruction\"]),\n",
    "        \"input\": \"\",\n",
    "        \"output\": normalize(example[\"output\"])\n",
    "    }\n",
    "\n",
    "oa_cleaned = oa_filtered.map(basic_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33215822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c35d8f99314ff6a78ab46fc527b16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After length filtering: 1139\n"
     ]
    }
   ],
   "source": [
    "## remove very short or use less rows\n",
    "\n",
    "def length_filter(example):\n",
    "    return len(example[\"instruction\"]) > 15 and len(example[\"output\"]) > 30\n",
    "\n",
    "oa_cleaned = oa_cleaned.filter(length_filter)\n",
    "\n",
    "print(\"After length filtering:\", len(oa_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82834982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d166618918ef46aa8fc1daf3d85dcd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After noise removal: 1108\n"
     ]
    }
   ],
   "source": [
    "## remove Non- Interview Style Content \n",
    "\n",
    "bad_patterns = [\"joke\", \"story\", \"poem\", \"translate\", \"email\", \"lyrics\"]\n",
    "\n",
    "def remove_noise(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return not any(p in text for p in bad_patterns)\n",
    "\n",
    "oa_cleaned = oa_cleaned.filter(remove_noise)\n",
    "\n",
    "print(\"After noise removal:\", len(oa_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da602bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee58aa8882c4d29ad19b02e068bfad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create Final training prompt feild \n",
    "\n",
    "def create_prompt(example):\n",
    "    text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "oa_final = oa_cleaned.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0bcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned OpenAssistant rows: 1108\n",
      "{'instruction': 'Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?', 'input': '', 'output': 'Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in. The point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart. This method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.', 'text': '### Instruction:\\nCan you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\\n\\n### Response:\\nSure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in. The point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart. This method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned OpenAssistant rows:\", len(oa_final))\n",
    "print(oa_final[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60fe07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a88412edb8d4e98b5fe3de19ef8e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## saving the cleaned dataset \n",
    "oa_final.save_to_disk(\"openassistant_ds_cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b0455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754e33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852eec52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd28c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb6b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a04cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a63c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2718ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd3325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd3c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c97f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cff3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9453bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aca76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29243199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fa7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd148c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
