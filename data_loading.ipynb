{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1373027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca \n",
    "# OpenAssistant \n",
    "# kaggle \n",
    "# Synthetic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb381e5",
   "metadata": {},
   "source": [
    "#### Loading dataset From alpaca -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969c403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/arun4/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e5a55b691f4e9abc43418281f2a439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c471e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2caba818",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"data science\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural network\",\n",
    "    \"statistics\",\n",
    "    \"probability\",\n",
    "    \"python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"sql\",\n",
    "    \"nlp\",\n",
    "    \"computer vision\",\n",
    "    \"regression\",\n",
    "    \"classification\",\n",
    "    \"clustering\",\n",
    "    \"model\",\n",
    "    \"overfitting\",\n",
    "    \"underfitting\",\n",
    "    \"gradient descent\",\n",
    "    \"interview\",\n",
    "    \"data analysis\",\n",
    "    \"feature engineering\",\n",
    "    \"data preprocessing\",\n",
    "    \"data visualization\",\n",
    "    \"decision tree\",\n",
    "    \"random forest\",\n",
    "    \"xgboost\",\n",
    "    \"transformer\",\n",
    "    \"llm\",\n",
    "    \"mlops\",\n",
    "    \"hyperparameter\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f337dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ds_related(example):\n",
    "    instruction = example[\"instruction\"].lower()\n",
    "    \n",
    "    return any(keyword in instruction for keyword in keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33219de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arun4\\.cache\\huggingface\\datasets\\tatsu-lab___parquet\\tatsu-lab--alpaca-2b32f0433506ef5f\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-0e70526648b7e046.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "\n",
    "filtered_data = train_data.filter(filter_ds_related)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41d37887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 52002\n",
      "Filtered size: 2226\n"
     ]
    }
   ],
   "source": [
    "print(\"Original size:\", len(train_data))\n",
    "print(\"Filtered size:\", len(filtered_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bbdb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 2226\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93780007",
   "metadata": {},
   "source": [
    "##### cleaning the filtered data set  -->\n",
    "Remove noisy records\n",
    "\n",
    "Fix formatting issues\n",
    "\n",
    "Remove very short/very long samples\n",
    "\n",
    "Remove bad quality answers\n",
    "\n",
    "Standardize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08b8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def basic_clean(example):\n",
    "    def clean_text(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    return {\n",
    "        \"instruction\": clean_text(example[\"instruction\"]),\n",
    "        \"input\": clean_text(example[\"input\"]) if example[\"input\"] else \"\",\n",
    "        \"output\": clean_text(example[\"output\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0198349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arun4\\.cache\\huggingface\\datasets\\tatsu-lab___parquet\\tatsu-lab--alpaca-2b32f0433506ef5f\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-e130327abbd68cd9.arrow\n"
     ]
    }
   ],
   "source": [
    "cleaned = filtered_data.map(basic_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4afcf38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arun4\\.cache\\huggingface\\datasets\\tatsu-lab___parquet\\tatsu-lab--alpaca-2b32f0433506ef5f\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-f31d3a7510bbaca9.arrow\n"
     ]
    }
   ],
   "source": [
    "## Remove Low Quality Records\n",
    "\n",
    "def remove_empty(example):\n",
    "    return len(example[\"instruction\"]) > 10 and len(example[\"output\"]) > 10\n",
    "\n",
    "cleaned = cleaned.filter(remove_empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6b4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Very Short or Very Long Samples\n",
    "\n",
    "def length_filter(example):\n",
    "    total_len = len(example[\"instruction\"]) + len(example[\"output\"])\n",
    "    return 30 < total_len < 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae70be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arun4\\.cache\\huggingface\\datasets\\tatsu-lab___parquet\\tatsu-lab--alpaca-2b32f0433506ef5f\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-fed9665589cf2b00.arrow\n"
     ]
    }
   ],
   "source": [
    "cleaned = cleaned.filter(length_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af32ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication: 2190\n"
     ]
    }
   ],
   "source": [
    "## Remove Duplicates\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to pandas\n",
    "df = cleaned.to_pandas()\n",
    "\n",
    "# Remove duplicate rows based on instruction + output\n",
    "df = df.drop_duplicates(subset=[\"instruction\", \"output\"])\n",
    "\n",
    "# Convert back to HuggingFace dataset\n",
    "cleaned_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(\"After deduplication:\", len(cleaned_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2616b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230113e497ff4468824b3cf1bb6f5105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create Final Prompt Format\n",
    "\n",
    "def create_prompt(example):\n",
    "    if example[\"input\"]:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{example[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\"\"\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "final_cleaned = cleaned_dataset.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05979378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned size: 2190\n",
      "{'instruction': 'Render a 3D model of a house', 'input': '', 'output': '<nooutput> This type of instruction cannot be fulfilled by a GPT model.', 'text': '### Instruction:\\nRender a 3D model of a house\\n\\n### Response:\\n<nooutput> This type of instruction cannot be fulfilled by a GPT model.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned size:\", len(final_cleaned))\n",
    "print(final_cleaned[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1749dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Non-Textual / Non-Answerable Tasks\n",
    "\n",
    "bad_keywords = [\n",
    "    \"render\",\n",
    "    \"draw\",\n",
    "    \"paint\",\n",
    "    \"image of\",\n",
    "    \"picture of\",\n",
    "    \"3d model\",\n",
    "    \"generate an image\",\n",
    "    \"create a video\",\n",
    "    \"audio file\",\n",
    "    \"physical\",\n",
    "    \"real world action\"\n",
    "]\n",
    "\n",
    "def remove_bad_tasks(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return not any(k in text for k in bad_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d81e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee8007aff1b47629bf7d1ce3256d78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing non-text tasks: 2179\n"
     ]
    }
   ],
   "source": [
    "cleaned = final_cleaned.filter(remove_bad_tasks)\n",
    "\n",
    "print(\"After removing non-text tasks:\", len(cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "150d2d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cba93da8b7449295f98dd95266b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned.save_to_disk(\"alpaca_ds_cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a486b5b",
   "metadata": {},
   "source": [
    "Loading open-assistant dataset -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51a219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/arun4/.cache/huggingface/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23dc779f0ee45ffa273729667a11e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "        num_rows: 84437\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "        num_rows: 4401\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load OpenAssistant dataset\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\")\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213afc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'parent_id': None, 'user_id': 'c3fe8c76-fc30-4fa7-b7f8-c492f5967d18', 'created_date': '2023-02-05T14:23:50.983374+00:00', 'text': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.', 'role': 'prompter', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': None, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.00044308538781479, 'severe_toxicity': 3.252684837207198e-05, 'obscene': 0.00023475120542570949, 'identity_attack': 0.0001416115992469713, 'insult': 0.00039489680784754455, 'threat': 4.075629112776369e-05, 'sexual_explicit': 2.712695459194947e-05}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': {'name': ['+1', '_skip_reply', '_skip_ranking'], 'count': [10, 1, 4]}, 'labels': {'name': ['spam', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9166666666666666, 0.16666666666666666, 0.3333333333333333, 0.6666666666666666, 0.0], 'count': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}}\n",
      "{'message_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c', 'parent_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'user_id': '2c96e467-66f0-4be7-9693-bda51356a424', 'created_date': '2023-02-06T13:50:44.657083+00:00', 'text': '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.', 'role': 'assistant', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': 0, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.00026396565954200923, 'severe_toxicity': 2.7454958399175666e-05, 'obscene': 0.00013584605767391622, 'identity_attack': 9.263094398193061e-05, 'insult': 0.0001668655313551426, 'threat': 3.769186878344044e-05, 'sexual_explicit': 2.500762275303714e-05}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': {'name': ['+1', '_skip_labeling'], 'count': [3, 1]}, 'labels': {'name': ['spam', 'fails_task', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'helpfulness', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9166666666666666, 0.375, 0.375, 0.75, 0.375, 0.0], 'count': [3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2]}}\n",
      "{'message_id': '6708c47f-05c9-4346-b3d2-40b2bd24fde4', 'parent_id': 'c8e83833-ecbc-44fe-b6db-735228c25a1c', 'user_id': '2c96e467-66f0-4be7-9693-bda51356a424', 'created_date': '2023-02-06T18:48:49.391686+00:00', 'text': 'Now explain it to a dog', 'role': 'prompter', 'lang': 'en', 'review_count': 3, 'review_result': True, 'deleted': False, 'rank': None, 'synthetic': False, 'model_name': None, 'detoxify': {'toxicity': 0.03648477792739868, 'severe_toxicity': 5.486844383995049e-05, 'obscene': 0.0003762090636882931, 'identity_attack': 0.0002415566414128989, 'insult': 0.013612336479127407, 'threat': 0.0017075861105695367, 'sexual_explicit': 0.00010235361696686596}, 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb', 'tree_state': 'ready_for_export', 'emojis': None, 'labels': {'name': ['spam', 'lang_mismatch', 'pii', 'not_appropriate', 'hate_speech', 'sexual_content', 'quality', 'toxicity', 'humor', 'creativity', 'violence'], 'value': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.5, 0.625, 0.5, 0.0], 'count': [3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2]}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"train\"][1])\n",
    "print(dataset[\"train\"][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bd924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompter', 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "roles = set(dataset[\"train\"][\"role\"])\n",
    "print(roles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7b55a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 prompter\n",
      "1 assistant\n",
      "2 prompter\n",
      "3 assistant\n",
      "4 prompter\n",
      "5 assistant\n",
      "6 prompter\n",
      "7 assistant\n",
      "8 assistant\n",
      "9 assistant\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, dataset[\"train\"][i][\"role\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd55a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[\"train\"]  \n",
    "alpaca_style = []\n",
    "\n",
    "for i in range(len(data) - 1):\n",
    "    current = data[i]\n",
    "    next_msg = data[i + 1]\n",
    "\n",
    "    # Only take valid prompter â†’ assistant pairs\n",
    "    if current[\"role\"] == \"prompter\" and next_msg[\"role\"] == \"assistant\":\n",
    "        alpaca_style.append({\n",
    "            \"instruction\": current[\"text\"].strip(),\n",
    "            \"input\": \"\",\n",
    "            \"output\": next_msg[\"text\"].strip()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372e1f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 27904\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# convert to hugging face dataset \n",
    "from datasets import Dataset\n",
    "\n",
    "oa_dataset = Dataset.from_list(alpaca_style)\n",
    "\n",
    "print(oa_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3584f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"data science\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural network\",\n",
    "    \"statistics\",\n",
    "    \"probability\",\n",
    "    \"python\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"sql\",\n",
    "    \"nlp\",\n",
    "    \"computer vision\",\n",
    "    \"regression\",\n",
    "    \"classification\",\n",
    "    \"clustering\",\n",
    "    \"model\",\n",
    "    \"overfitting\",\n",
    "    \"underfitting\",\n",
    "    \"gradient descent\",\n",
    "    \"interview\",\n",
    "    \"data analysis\",\n",
    "    \"feature engineering\",\n",
    "    \"data preprocessing\",\n",
    "    \"data visualization\",\n",
    "    \"decision tree\",\n",
    "    \"random forest\",\n",
    "    \"xgboost\",\n",
    "    \"transformer\",\n",
    "    \"llm\",\n",
    "    \"mlops\",\n",
    "    \"hyperparameter\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7a3ec",
   "metadata": {},
   "source": [
    "filtering rows for data science domain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4598ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f8c6889e114b99805749a0bedc7f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after DS filtering: 1165\n"
     ]
    }
   ],
   "source": [
    "def filter_ds_related(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "oa_filtered = oa_dataset.filter(filter_ds_related)\n",
    "\n",
    "print(\"Rows after DS filtering:\", len(oa_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b00549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adb921e3afb4f39a8ae3144056b800c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## basic cleaning - remove extra space and normalize text \n",
    "\n",
    "import re\n",
    "\n",
    "def basic_clean(example):\n",
    "    def normalize(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text)   # remove multiple spaces\n",
    "        return text\n",
    "\n",
    "    return {\n",
    "        \"instruction\": normalize(example[\"instruction\"]),\n",
    "        \"input\": \"\",\n",
    "        \"output\": normalize(example[\"output\"])\n",
    "    }\n",
    "\n",
    "oa_cleaned = oa_filtered.map(basic_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33215822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c35d8f99314ff6a78ab46fc527b16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After length filtering: 1139\n"
     ]
    }
   ],
   "source": [
    "## remove very short or use less rows\n",
    "\n",
    "def length_filter(example):\n",
    "    return len(example[\"instruction\"]) > 15 and len(example[\"output\"]) > 30\n",
    "\n",
    "oa_cleaned = oa_cleaned.filter(length_filter)\n",
    "\n",
    "print(\"After length filtering:\", len(oa_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82834982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d166618918ef46aa8fc1daf3d85dcd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After noise removal: 1108\n"
     ]
    }
   ],
   "source": [
    "## remove Non- Interview Style Content \n",
    "\n",
    "bad_patterns = [\"joke\", \"story\", \"poem\", \"translate\", \"email\", \"lyrics\"]\n",
    "\n",
    "def remove_noise(example):\n",
    "    text = example[\"instruction\"].lower()\n",
    "    return not any(p in text for p in bad_patterns)\n",
    "\n",
    "oa_cleaned = oa_cleaned.filter(remove_noise)\n",
    "\n",
    "print(\"After noise removal:\", len(oa_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da602bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee58aa8882c4d29ad19b02e068bfad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create Final training prompt feild \n",
    "\n",
    "def create_prompt(example):\n",
    "    text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "oa_final = oa_cleaned.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0bcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned OpenAssistant rows: 1108\n",
      "{'instruction': 'Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?', 'input': '', 'output': 'Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in. The point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart. This method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.', 'text': '### Instruction:\\nCan you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?\\n\\n### Response:\\nSure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and a dog picture could have very similar representations (their numbers would be close to each other), while at others two cat images may be represented far apart. In simple terms, the model wouldn\\'t be able to tell cats and dogs apart. This is where contrastive learning comes in. The point of contrastive learning is to take pairs of samples (in this case images of cats and dogs), then train the model to \"pull\" representations of similar pairs (cat-cat or dog-dog) closer to each other and \"push\" representations of different pairs (cat-dog) apart. After doing this for a sufficient number of steps, your model will be able to produce unique, reliable representations for cats and dogs, in essence tell them apart. This method is not limited to images, you can typically use it with any dataset that has similar and dissimilar data points.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned OpenAssistant rows:\", len(oa_final))\n",
    "print(oa_final[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60fe07d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a88412edb8d4e98b5fe3de19ef8e199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## saving the cleaned dataset \n",
    "oa_final.save_to_disk(\"openassistant_ds_cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db537944",
   "metadata": {},
   "source": [
    "Loading Kaggle ml_interview_questions -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227b0455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/arun4/.cache/huggingface/datasets/csv/default-3bc42a4a125c2dfc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a380bd03da4d0b8185b3773468baf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6ba2fac21d4e078ae3e789ad73ac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d2f71f51a34ce99d81bc8726740609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/arun4/.cache/huggingface/datasets/csv/default-3bc42a4a125c2dfc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0164b34bacb94ec68e01a90e7c361e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'answer', 'category', 'difficulty', 'company_tags', 'topic_tags', 'answer_length'],\n",
      "    num_rows: 502\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"ml_interview_questions.csv\")\n",
    "\n",
    "data = dataset[\"train\"]\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversion funtion - to convert in alpaca format \n",
    "\n",
    "def convert_to_alpaca(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"].strip(),\n",
    "        \"input\": \"\",\n",
    "        \"output\": example[\"answer\"].strip()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852eec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\arun4\\.cache\\huggingface\\datasets\\csv\\default-3bc42a4a125c2dfc\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-5594d89436c359b2.arrow\n"
     ]
    }
   ],
   "source": [
    "kaggle_ds = data.map(\n",
    "    convert_to_alpaca,\n",
    "    remove_columns=data.column_names\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd28c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaee0b6d6f10407b9ed8e447a3b5333f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Text Prompt Field\n",
    "def create_prompt(example):\n",
    "    text = f\"\"\"### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "kaggle_ds = kaggle_ds.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214deaf",
   "metadata": {},
   "source": [
    "Basic cleaning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fb6b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e05e410f5d484ca28f46a42c01a455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Remove very small rows\n",
    "\n",
    "def quality_filter(example):\n",
    "    return len(example[\"instruction\"]) > 10 and len(example[\"output\"]) > 20\n",
    "\n",
    "kaggle_ds = kaggle_ds.filter(quality_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a04cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Describe the ConvNeXt architecture and its key innovations.', 'input': '', 'output': 'ConvNeXt is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.', 'text': '### Instruction:\\nDescribe the ConvNeXt architecture and its key innovations.\\n\\n### Response:\\nConvNeXt is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.'}\n",
      "{'instruction': 'What is data augmentation and why is it important for computer vision?', 'input': '', 'output': 'Data augmentation applies random transformations to training images (rotation, flipping, cropping, color jittering, scaling) to increase effective training set size and improve generalization. It reduces overfitting and makes models invariant to transformations. Advanced: CutOut, CutMix, MixUp, AutoAugment, RandAugment. For certain tasks like medical imaging, augmentation is critical due to limited labeled data. Test-time augmentation (TTA) averages predictions across augmented copies at inference.', 'text': '### Instruction:\\nWhat is data augmentation and why is it important for computer vision?\\n\\n### Response:\\nData augmentation applies random transformations to training images (rotation, flipping, cropping, color jittering, scaling) to increase effective training set size and improve generalization. It reduces overfitting and makes models invariant to transformations. Advanced: CutOut, CutMix, MixUp, AutoAugment, RandAugment. For certain tasks like medical imaging, augmentation is critical due to limited labeled data. Test-time augmentation (TTA) averages predictions across augmented copies at inference.'}\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_ds[0])\n",
    "print(kaggle_ds[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45a24a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "print(len(kaggle_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a63c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686e13ed821d45d68d31d9e0fcb84d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kaggle_ds.save_to_disk(\"kaggle_interview_cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb264e",
   "metadata": {},
   "source": [
    "Loading some synthetic data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2718ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 shape: (500, 3)\n",
      "Dataset 2 shape: (500, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load synthetic datasets\n",
    "ds1 = pd.read_csv(\"ds_instruction_dataset_1.csv\")\n",
    "ds2 = pd.read_csv(\"ds_instruction_dataset_2.csv\")\n",
    "\n",
    "print(\"Dataset 1 shape:\", ds1.shape)\n",
    "print(\"Dataset 2 shape:\", ds2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91dd3325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['instruction', 'input', 'output'], dtype='object')\n",
      "Index(['instruction', 'input', 'output'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ds1.columns)\n",
    "print(ds2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2fd3c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total synthetic rows: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "## combining both datasets \n",
    "synthetic = pd.concat([ds1, ds2], ignore_index=True)\n",
    "\n",
    "print(\"Total synthetic rows:\", synthetic.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating Prompt Template\n",
    "\n",
    "def create_prompt(row):\n",
    "    return f\"\"\"### Instruction:\n",
    "{row['instruction']}\n",
    "\n",
    "### Question:\n",
    "{row['input']}\n",
    "\n",
    "### Answer:\n",
    "{row['output']}\n",
    "\"\"\"\n",
    "\n",
    "synthetic[\"text\"] = synthetic.apply(create_prompt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bddeda93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does neural networks help in data science ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neural networks helps by improving accuracy, e...</td>\n",
       "      <td>### Instruction:\\nHow does neural networks hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain the concept of MLOps.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The concept of MLOps refers to fundamental pri...</td>\n",
       "      <td>### Instruction:\\nExplain the concept of MLOps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is algorithms and why is it important?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithms is a key area in data science. It h...</td>\n",
       "      <td>### Instruction:\\nWhat is algorithms and why i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What mistakes should be avoided in MLOps?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Common mistakes in MLOps include poor understa...</td>\n",
       "      <td>### Instruction:\\nWhat mistakes should be avoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain python in beginner friendly language.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python can be understood as a way to analyze a...</td>\n",
       "      <td>### Instruction:\\nExplain python in beginner f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  input  \\\n",
       "0  How does neural networks help in data science ...    NaN   \n",
       "1                      Explain the concept of MLOps.    NaN   \n",
       "2        What is algorithms and why is it important?    NaN   \n",
       "3          What mistakes should be avoided in MLOps?    NaN   \n",
       "4      Explain python in beginner friendly language.    NaN   \n",
       "\n",
       "                                              output  \\\n",
       "0  neural networks helps by improving accuracy, e...   \n",
       "1  The concept of MLOps refers to fundamental pri...   \n",
       "2  algorithms is a key area in data science. It h...   \n",
       "3  Common mistakes in MLOps include poor understa...   \n",
       "4  python can be understood as a way to analyze a...   \n",
       "\n",
       "                                                text  \n",
       "0  ### Instruction:\\nHow does neural networks hel...  \n",
       "1  ### Instruction:\\nExplain the concept of MLOps...  \n",
       "2  ### Instruction:\\nWhat is algorithms and why i...  \n",
       "3  ### Instruction:\\nWhat mistakes should be avoi...  \n",
       "4  ### Instruction:\\nExplain python in beginner f...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f5c97f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfd5f6b25d44d4aa3fc33073e574223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  synthetic is currently a pandas DataFrame, not a Hugging Face Dataset.\n",
    "##  coverting pandas dataframe into huggingface dataset \n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "synthetic_ds = Dataset.from_pandas(synthetic)\n",
    "\n",
    "synthetic_ds.save_to_disk(\"synthetic_ds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e4bed",
   "metadata": {},
   "source": [
    "Loading all datasets -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5cff3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 2179\n",
      "})\n",
      "OpenAssistant: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 1108\n",
      "})\n",
      "Kaggle: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 502\n",
      "})\n",
      "Synthetic: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "alpaca = load_from_disk(\"alpaca_ds_cleaned\")\n",
    "oasst = load_from_disk(\"openassistant_ds_cleaned\")\n",
    "kaggle = load_from_disk(\"kaggle_interview_cleaned\")\n",
    "synthetic = load_from_disk(\"synthetic_ds\")\n",
    "\n",
    "print(\"Alpaca:\", alpaca)\n",
    "print(\"OpenAssistant:\", oasst)\n",
    "print(\"Kaggle:\", kaggle)\n",
    "print(\"Synthetic:\", synthetic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5193e8",
   "metadata": {},
   "source": [
    "Before concatenating, we must standardize all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function \n",
    "def fix_types(dataset):\n",
    "    return dataset.map(lambda x: {\n",
    "        \"instruction\": str(x[\"instruction\"]),\n",
    "        \"input\": str(x[\"input\"]),\n",
    "        \"output\": str(x[\"output\"]),\n",
    "        \"text\": str(x[\"text\"])\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d7cd117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97450c19875a443fa7fa5cc9bd46f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724b545f24b64797ba333e3ebd2161c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b152238c1254732a596bfe076fb9572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b913256934f40feabe516736da8bba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca = fix_types(alpaca)\n",
    "oasst = fix_types(oasst)\n",
    "kaggle = fix_types(kaggle)\n",
    "synthetic = fix_types(synthetic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682cce4",
   "metadata": {},
   "source": [
    "Concatinating the datasets -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29aca76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 4789\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "final_dataset = concatenate_datasets([alpaca, oasst, kaggle, synthetic])\n",
    "\n",
    "print(\"Final dataset size:\", final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29243199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle the dataset \n",
    "\n",
    "final_dataset = final_dataset.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fa7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 4310\n",
      "})\n",
      "Test size: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 479\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## Create Train / Test Split\n",
    "\n",
    "\n",
    "split = final_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split[\"train\"]\n",
    "test_dataset = split[\"test\"]\n",
    "\n",
    "print(\"Train size:\", train_dataset)\n",
    "print(\"Test size:\", test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d05dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'input', 'output', 'text']\n",
      "['instruction', 'input', 'output', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)\n",
    "print(test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the Unnecessary columns \n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "test_dataset = test_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "730e4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n",
      "['text']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)\n",
    "print(test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcf857",
   "metadata": {},
   "source": [
    "The final train and test datasets are loaded .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b563e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9141b2e36634e0d86066fb90a65c47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001a4c7719ec408f8e63d0abb6a09651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk(\"train_dataset\")\n",
    "test_dataset.save_to_disk(\"test_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f18d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c7bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
