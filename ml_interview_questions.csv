"id","question","answer","category","difficulty","company_tags","topic_tags","answer_length"
"aa928e5348","Describe the ConvNeXt architecture and its key innovations.","ConvNeXt is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Amazon|Databricks","architecture|deep learning","39"
"e940376e2c","What is data augmentation and why is it important for computer vision?","Data augmentation applies random transformations to training images (rotation, flipping, cropping, color jittering, scaling) to increase effective training set size and improve generalization. It reduces overfitting and makes models invariant to transformations. Advanced: CutOut, CutMix, MixUp, AutoAugment, RandAugment. For certain tasks like medical imaging, augmentation is critical due to limited labeled data. Test-time augmentation (TTA) averages predictions across augmented copies at inference.","Computer Vision","easy","OpenAI|Meta|Databricks|Robinhood","data augmentation|training|regularization","62"
"a0d8ce2738","How would you design a medical diagnosis support system?","Designing a medical diagnosis support system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Bloomberg|Google","system design|production","42"
"0991ad7b39","Explain neural network pruning in deep learning.","Pruning removes unnecessary weights/neurons to compress models. Types: (1) Unstructured: remove individual weights (sparse matrices, needs special hardware). (2) Structured: remove entire filters/heads/layers (maintains dense computation). Methods: magnitude pruning, movement pruning, lottery ticket hypothesis. Schedule: one-shot vs. iterative (prune-retrain cycles). Typically 50-90% of weights can be removed with minimal accuracy loss. Combined with quantization and distillation for maximum compression.","Deep Learning","hard","Pinterest|Tesla|Microsoft","model compression|efficiency|deployment","59"
"17b12e11f9","How would you implement a custom PyTorch Dataset with efficient data loading?","class CustomDataset(torch.utils.data.Dataset): def __init__(self, data_path): self.data = pd.read_parquet(data_path); def __len__(self): return len(self.data); def __getitem__(self, idx): row = self.data.iloc[idx]; return self.transform(row). For efficiency: (1) use memory-mapped files (np.memmap) or HDF5 for large data, (2) pre-compute features, (3) use num_workers > 0 in DataLoader (multiprocessing), (4) pin_memory=True for GPU transfer, (5) prefetch_factor for overlap, (6) persistent_workers to avoid re-init. For images: read file paths in __init__, load images lazily in __getitem__.","Python","hard","Citadel|Snowflake|Goldman Sachs","PyTorch|data loading|performance","69"
"f61dd02803","Describe the PaLM architecture and its key innovations.","PaLM is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Twitter|ByteDance|Salesforce","architecture|deep learning","39"
"5e7ff1738d","Explain text summarization approaches.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Two Sigma|Databricks|Stripe","NLP|language understanding","32"
"baeb9e6806","Explain the Kendall tau and when you would use it.","The Kendall tau is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Apple|LinkedIn","statistical tests|hypothesis testing","63"
"039ee18c51","Explain the scaling laws for language models.","Scaling laws (Kaplan et al., Chinchilla) show that LLM loss decreases as a power law with model size (N), dataset size (D), and compute (C): L(N,D) ~ a/N^alpha + b/D^beta + L_irreducible. Key findings: (1) performance improves predictably with scale, (2) compute-optimal training (Chinchilla) requires scaling data and parameters equally -- roughly 20 tokens per parameter, (3) emergent abilities appear at certain scales, (4) these laws guide resource allocation for training large models.","NLP","hard","Lyft|Airbnb|Google","language models|scaling|training","73"
"327f7430ac","Explain learning curves and its importance in machine learning.","Learning Curves is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","easy","Walmart|JPMorgan|LinkedIn","machine learning|practical ML","49"
"614a963127","What is Bayesian inference? How does it differ from frequentist inference?","Bayesian inference treats parameters as random variables with prior distributions, updating beliefs via Bayes' theorem: P(theta|data) proportional to P(data|theta) * P(theta). Frequentist inference treats parameters as fixed unknowns and focuses on the sampling distribution of estimators. Key differences: Bayesians incorporate prior knowledge; frequentists rely solely on data. Bayesians produce posterior distributions; frequentists produce point estimates and confidence intervals.","Statistics","medium","Doordash|ByteDance","bayesian statistics|inference","58"
"bb95151077","What is bias in ML models?","Statistical bias: E[estimate]-true_value. Algorithmic bias: systematic unfairness across groups. Data bias: unrepresentative training data. Types: selection, measurement, historical, representation, label bias. Mitigation: diverse data, fairness constraints, bias audits.","ML Theory","medium","Two Sigma|Adobe","fairness|bias|ethics","28"
"b5e4eb3673","Explain Python packaging for ML projects in Python.","Understanding Python packaging for ML projects is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","easy","Spotify|DeepMind","Python|practical skills","31"
"bc8e7230be","Design a recommendation system for an e-commerce platform.","Architecture: (1) Candidate generation: collaborative filtering (matrix factorization, ALS), content-based (item features), two-tower models (user/item embeddings). (2) Ranking: gradient-boosted trees or deep ranking model using user features, item features, context (time, device). (3) Re-ranking: diversity, freshness, business rules. Infrastructure: feature store for user/item features, real-time events via Kafka, model serving with low latency (<100ms). Evaluation: offline (NDCG, recall@k), online (CTR, revenue, engagement A/B tests).","System Design","medium","OpenAI|Snowflake|Bloomberg","recommendation systems|system design|ranking","64"
"3f7aa23fdf","What is the difference between teacher forcing and free running in seq2seq?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","DeepMind|Citadel","deep learning|architecture","38"
"65c3ccf53a","Explain dependency parsing.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","LinkedIn|Citadel|Stripe","NLP|language understanding","32"
"c951a20629","What is Bayesian A/B testing?","This is an important concept in a/b testing that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","A/B Testing","medium","Meta|Apple|Jane Street","bayesian|experimentation","31"
"d160e6a6ef","Explain the ROC curve and AUC. When is AUC-PR preferred?","ROC plots True Positive Rate (recall) vs. False Positive Rate (1-specificity) at different classification thresholds. AUC-ROC summarizes overall discriminative ability (0.5 = random, 1.0 = perfect). AUC-PR (Precision-Recall) is preferred when classes are highly imbalanced because ROC can be overly optimistic -- a model that catches most positives but flags many negatives still shows good ROC but poor precision.","ML Theory","medium","Apple|Meta","classification metrics|evaluation|imbalanced data","59"
"9a64987473","Explain stored procedures in SQL.","Understanding stored procedures is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Spotify|Google|Meta","SQL|database","31"
"64180eab44","Explain pose estimation.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","hard","Adobe|OpenAI|Microsoft","computer vision|tasks","30"
"5abfac113e","Explain the concept of representation learning.","This is an important concept in deep learning that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","Deep Learning","hard","Uber|Amazon|Netflix","representation|fundamentals","31"
"a26a6f83cc","What is ONNX and why is it useful for model deployment?","This is an important concept in system design that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","System Design","hard","Meta|LinkedIn","deployment|optimization","31"
"936f37f318","Explain asyncio.","Asynchronous I/O with async/await. Event loop manages concurrent tasks without threads. Use for I/O-bound operations (HTTP, DB, files). Not for CPU-bound (use multiprocessing). Key for API serving (FastAPI).","Python","medium","DeepMind|Databricks","async|concurrency","28"
"d05d26ee36","Explain type hints in Python.","Annotate types: def train(model: Module, lr: float) -> Dict[str, float]. Benefits: documentation, IDE autocomplete, static checking (mypy), catch bugs. ML-specific: tensor shapes (jaxtyping). No runtime overhead.","Python","medium","Meta|Instacart","type hints|code quality","26"
"f0228545e2","What is the YOLO family of object detectors? Compare key versions.","YOLO (You Only Look Once) frames detection as a single regression problem. YOLOv1: single pass, grid-based prediction. YOLOv2/3: multi-scale detection, anchor boxes, Darknet backbone. YOLOv4/5: CSPNet backbone, mosaic augmentation, PANet neck. YOLOv7/8: efficient layer aggregation, anchor-free options, improved training strategies. YOLO prioritizes speed over accuracy compared to two-stage detectors. Widely used in real-time applications: autonomous driving, surveillance, robotics.","Computer Vision","medium","Netflix|ByteDance|JPMorgan|Lyft","object detection|real-time|architecture","58"
"bac446d040","Explain the GROUPING SETS, CUBE, and ROLLUP operations.","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","Stripe|Google","SQL|analytics","27"
"ab213d120a","What is the difference between staticmethod and classmethod?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","hard","ByteDance|Microsoft|Instacart","Python|advanced","26"
"065a0a7b16","Explain ROUGE metrics.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","hard","Amazon|Two Sigma|OpenAI","NLP|language understanding","32"
"b938ef7b18","What are decorators in Python? Write a timing decorator.","Decorators are functions that modify other functions' behavior. They take a function as input and return a new function. Example timing decorator: import time; def timer(func): def wrapper(*args, **kwargs): start = time.time(); result = func(*args, **kwargs); print(f'{func.__name__} took {time.time()-start:.4f}s'); return result; return wrapper. Usage: @timer def train(...). Decorators are used for logging, caching (@lru_cache), authentication, retry logic, and registering functions.","Python","medium","Meta|Google","decorators|functions|design patterns","60"
"9fb0586f21","When and how do you use the augmented Dickey-Fuller test?","The augmented Dickey-Fuller test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","easy","Two Sigma|LinkedIn|Doordash","statistical tests|hypothesis testing","45"
"0e6ca44569","How would you design a predictive maintenance system?","Designing a predictive maintenance system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","ByteDance|Snowflake|Apple","system design|production","41"
"13fe42f642","Explain database normalization.","1NF: atomic values. 2NF: no partial deps. 3NF: no transitive deps. Benefits: integrity, less storage. Denormalize for read-heavy workloads, data warehousing (star schema). Balance: normalize OLTP, denormalize OLAP.","SQL","medium","Meta|Microsoft|Bloomberg","normalization|database design","28"
"90421b3fe7","Explain feature crosses and when to use them.","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Snowflake|Apple","feature engineering|data preprocessing","33"
"d80ca52949","How do you handle high-cardinality categorical features?","Strategies: (1) Target encoding with regularization. (2) Frequency/count encoding. (3) Hash encoding: fixed-size representation. (4) Embedding layers (deep learning). (5) Grouping rare categories into 'Other'. (6) Clustering categories by target distribution. (7) Leave-one-out encoding. (8) Binary encoding. Avoid one-hot for high cardinality. Tree-based models handle ordinal encoding well. For neural networks, embeddings are standard.","Feature Engineering","medium","Google|Uber|Apple","categorical encoding|high cardinality|feature engineering","54"
"fc1be89fc9","What is the difference between Type I and Type II errors?","A Type I error (false positive) occurs when we reject a true null hypothesis. A Type II error (false negative) occurs when we fail to reject a false null hypothesis. The probability of a Type I error is denoted alpha (significance level), and the probability of a Type II error is denoted beta. Power = 1 - beta. There is a trade-off: lowering alpha increases beta and vice versa.","Statistics","easy","Google|Doordash|Walmart|Apple","hypothesis testing|error analysis","69"
"e7f31d886c","When and how do you use the Ljung-Box test?","The Ljung-Box test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","ByteDance|Netflix|Stripe","statistical tests|hypothesis testing","44"
"a2bca538ac","Describe the Whisper architecture and its key innovations.","Whisper is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Google|Meta|Netflix","architecture|deep learning","39"
"8af50b55b2","Explain the theoretical foundations of in-context learning in LLMs.","In-context learning (ICL) allows LLMs to perform tasks from examples in the prompt without parameter updates. Theoretical perspectives: (1) Transformers implicitly implement gradient descent on in-context examples (Akyurek et al., von Oswald et al.). (2) ICL as Bayesian inference: the model implicitly marginalizes over possible concepts. (3) Induction heads (Olsson et al.) -- attention patterns that copy and complete patterns. ICL quality depends on model scale, example quality, and format. Understanding is still an active research area.","NLP","hard","Instacart|Meta|Netflix|Amazon","language models|few-shot learning|transformers","77"
"271b07543a","What is image classification?","Assigns label to entire image. Evolution: LeNet, AlexNet, VGG, ResNet, EfficientNet, ViT. Training: cross-entropy, augmentation, transfer learning. Modern: fine-tune ViT/ConvNeXt rather than training from scratch.","Computer Vision","easy","Amazon|Goldman Sachs|Tesla","image classification|CNN","25"
"454c6228c5","Explain the BERT pre-training objectives and fine-tuning process.","BERT pre-training: (1) Masked Language Modeling (MLM) -- randomly mask 15% of tokens and predict them using bidirectional context. (2) Next Sentence Prediction (NSP) -- predict if two sentences are consecutive (later found less useful; RoBERTa drops it). Fine-tuning: add a task-specific head (e.g., linear classifier for classification, token classifier for NER), and train all parameters on labeled data with a small learning rate (2e-5 to 5e-5) for a few epochs.","NLP","medium","Microsoft|Doordash|Twitter","transformers|pre-training|fine-tuning","71"
"8f78ae9866","Explain Dirichlet distribution and topic modeling.","Dirichlet Distribution And Topic Modeling is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Samsung|Spotify|Google","probability|distributions","38"
"d610262c30","Explain query optimization. What are execution plans and indexes?","Query optimization involves analyzing and restructuring queries for better performance. Execution plan (EXPLAIN): shows the database's strategy for executing a query -- scan types, join methods, estimated costs. Indexes: B-tree (default, good for range queries), hash (equality), bitmap (low cardinality), composite (multi-column). Best practices: index columns used in WHERE/JOIN/ORDER BY, avoid SELECT *, minimize subqueries, use CTEs, avoid functions on indexed columns (prevents index usage).","SQL","medium","Instacart|Amazon|Google","optimization|indexing|performance","65"
"e79d3064f7","What is stratified k-fold cross-validation?","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","ByteDance|Salesforce|Goldman Sachs","evaluation|validation","31"
"58f82735df","Explain effect size and why it matters.","Effect size measures magnitude of a difference independent of sample size. Common measures: Cohen's d, correlation r, odds ratio, R-squared. Important because statistical significance alone does not indicate practical importance. Large samples can make tiny effects significant.","Statistics","medium","Meta|LinkedIn|Doordash","effect size|inference|practical significance","37"
"67555e3843","What is the difference between bagging and boosting?","Bagging (Bootstrap Aggregating) trains models independently on bootstrap samples and aggregates predictions -- reduces variance (e.g., Random Forest). Boosting trains models sequentially, each correcting the previous model's errors -- reduces bias (e.g., AdaBoost, Gradient Boosting). Bagging is easily parallelizable; boosting is sequential. Boosting can overfit if not regularized but typically achieves lower error.","ML Theory","medium","Goldman Sachs|Uber|Meta","ensemble methods|model selection","53"
"e37915f1f9","Explain Vision Transformers (ViT) and how they differ from CNNs.","ViT (Dosovitskiy et al., 2020) splits an image into fixed-size patches (e.g., 16x16), linearly embeds each patch, adds positional embeddings, and processes the sequence with a standard Transformer encoder. Key differences from CNNs: (1) global receptive field from the start (vs. local in CNNs), (2) no built-in translation equivariance, (3) requires more data or pre-training but scales better. Hybrid models (Swin Transformer) combine local and global attention. ViT family now matches or exceeds CNNs.","Computer Vision","medium","Google|Snowflake|Amazon","vision transformer|architecture|attention","74"
"8f08d1d382","What is the difference between population and sample?","A population includes all members of a defined group. A sample is a subset of the population selected for analysis. We use samples because collecting data from an entire population is often impractical. Key considerations include sampling bias, representativeness, and the trade-off between sample size and cost.","Statistics","easy","ByteDance|Amazon|Intel|DeepMind","descriptive statistics|sampling","47"
"ace92b6ebe","What is the difference between confidence intervals and credible intervals?","A 95% confidence interval means that if we repeated the experiment many times, 95% of the computed intervals would contain the true parameter. A 95% credible interval (Bayesian) means there is a 95% posterior probability the parameter lies within the interval. CIs are properties of the procedure; credible intervals are probability statements about the parameter given observed data and priors.","Statistics","medium","Two Sigma|Databricks|Walmart|Amazon","inference|bayesian statistics","60"
"e18e93468a","Explain handling large files efficiently in Python.","Understanding handling large files efficiently is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Salesforce|Amazon","Python|practical skills","30"
"1e59465555","Explain the difference between supervised and unsupervised learning.","Supervised learning uses labeled data (input-output pairs) to learn a mapping function. Examples: classification, regression. Unsupervised learning finds patterns in unlabeled data. Examples: clustering, dimensionality reduction, anomaly detection. Semi-supervised learning uses a small amount of labeled data with a large amount of unlabeled data. Self-supervised learning creates labels from the data itself.","ML Theory","easy","Palantir|Lyft|JPMorgan|Stripe","learning paradigms|fundamentals","52"
"4d3525cb5d","What is a convolutional neural network (CNN)?","A CNN uses convolutional layers that apply learnable filters across spatial dimensions to detect local patterns (edges, textures, objects). Key components: convolutional layers (local connectivity, weight sharing), pooling layers (spatial downsampling), fully connected layers (classification). Hierarchical feature learning: early layers detect edges, middle layers detect textures/parts, deep layers detect objects. Architectures: LeNet, AlexNet, VGG, ResNet, EfficientNet.","Computer Vision","easy","LinkedIn|Walmart","CNN|architecture|image classification","56"
"59e0db93a9","What is Simpson's paradox?","Simpson's paradox occurs when a trend in groups of data reverses when the groups are combined. Example: Drug A may have higher success rate in both men and women separately, but Drug B has higher rate overall due to unequal group sizes. Caused by confounding. Solution: stratified analysis.","Statistics","medium","Spotify|JPMorgan","paradoxes|confounding|causal inference","48"
"b5b294946d","Explain the Shapiro-Wilk test and when you would use it.","The Shapiro-Wilk test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","easy","Stripe|Databricks|Microsoft","statistical tests|hypothesis testing","63"
"ba166789c3","What is visual grounding?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","DeepMind|Tesla","computer vision|tasks","30"
"00c31bfa50","Explain multi-task learning.","Trains on multiple related tasks simultaneously, sharing representations. Benefits: better generalization, data efficiency, faster convergence. Architectures: hard sharing (shared layers) vs soft sharing (regularized). Challenge: negative transfer when tasks conflict.","ML Theory","medium","Citadel|Meta","multi-task|transfer learning","30"
"ec9c01d724","What is the lottery ticket hypothesis?","The lottery ticket hypothesis (Frankle and Carlin, 2019) states that dense randomly-initialized networks contain sparse subnetworks ('winning tickets') that, when trained in isolation from the same initialization, reach comparable accuracy in a similar number of iterations. Finding these tickets involves iterative magnitude pruning. Implications: explains why overparameterization helps, motivates structured pruning and sparse training. Extensions: late resetting, universal tickets, linear mode connectivity.","Deep Learning","hard","Uber|Bloomberg","pruning|network architecture|optimization","62"
"cd21986401","When would you choose Isolation Forest over One-Class SVM and vice versa?","Isolation Forest and One-Class SVM are both powerful algorithms but suited to different scenarios. Isolation Forest may be preferred when its assumptions match the data characteristics, while One-Class SVM excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Tesla|Spotify|Walmart","model comparison|model selection","63"
"d7941c1891","What is the Fisher Information Matrix and why is it important?","The Fisher Information Matrix I(theta) measures the amount of information that observable data carries about an unknown parameter. I(theta)_ij = -E[d^2 log L / d theta_i d theta_j]. It determines the Cramer-Rao lower bound on the variance of unbiased estimators, governs the asymptotic distribution of MLEs, and is used in natural gradient descent, information geometry, and optimal experimental design.","Statistics","hard","Doordash|Databricks|Microsoft","information theory|estimation|optimization","59"
"1831a1f779","What is a stratified experiment?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Jane Street|Google|Netflix","experimentation|methodology","36"
"eff5e0448d","What is feature pyramid network (FPN) and why is it important for detection?","FPN (Lin et al., 2017) builds a top-down architecture with lateral connections to create a multi-scale feature pyramid from a single-scale input. It combines low-resolution, semantically strong features with high-resolution, spatially precise features. This enables detection of objects at different scales. Used as a backbone/neck in Faster R-CNN, Mask R-CNN, RetinaNet. Extensions: PANet, BiFPN (EfficientDet), NAS-FPN.","Computer Vision","medium","ByteDance|Capital One|Uber","object detection|multi-scale|architecture","56"
"f0ae314a93","How do you handle data versioning in ML pipelines?","This is an important concept in system design that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","System Design","medium","DeepMind|Spotify","MLOps|data","31"
"2ce135089a","Explain contrastive learning for visual representations.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","easy","Apple|Palantir","computer vision|tasks","30"
"c207487a34","When and how do you use the one-way ANOVA?","The one-way ANOVA is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","hard","Amazon|Twitter|Snap","statistical tests|hypothesis testing","44"
"b564a9728d","What is distributional shift and how do you handle it?","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Amazon|Meta","deployment|drift","31"
"8cdb6ef648","Explain the mathematics behind deformable convolutions.","Standard convolution samples from a fixed grid R. Deformable convolution (Dai et al., 2017) adds learnable offsets Delta_p to each grid position: y(p0) = sum_{p_n in R} w(p_n) * x(p0 + p_n + Delta_p_n). Offsets are predicted by a separate conv layer. Since p0 + p_n + Delta_p_n is fractional, bilinear interpolation is used for differentiability. v2 adds modulation scalars. This enables adaptive receptive fields for geometric transformations, improving detection of non-rigid/unusual objects.","Computer Vision","hard","Meta|DeepMind|Google","convolution|architecture|geometric deep learning","73"
"50016f3723","Explain the mathematics behind Support Vector Machines (primal and dual formulations).","Primal: min_{w,b} (1/2)||w||^2 s.t. y_i(w.x_i + b) >= 1. Dual (via Lagrange multipliers alpha_i): max sum(alpha_i) - (1/2) sum_ij alpha_i alpha_j y_i y_j x_i.x_j, s.t. alpha_i >= 0, sum(alpha_i y_i) = 0. The dual reveals: (1) solution depends only on inner products (enabling kernel trick), (2) only support vectors (alpha_i > 0) define the boundary. Soft-margin: add slack variables and C parameter to handle non-separable data.","ML Theory","hard","Doordash|Microsoft|NVIDIA","SVM|optimization|kernel methods","66"
"91fc5effda","How do you pivot data in SQL?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Walmart|Snowflake","SQL|analytics","27"
"2b0abe6f4d","Explain the difference between CNNs and RNNs.","CNNs (Convolutional Neural Networks) use convolutional filters to capture local spatial patterns -- ideal for images, video, and grid-structured data. Key operations: convolution, pooling, stride. RNNs (Recurrent Neural Networks) process sequential data by maintaining a hidden state across time steps -- used for text, time series. RNNs suffer from vanishing gradients; LSTMs and GRUs address this. Transformers have largely replaced RNNs.","Deep Learning","easy","Amazon|Tesla","CNN|RNN|architecture","61"
"900b5112eb","Describe the ShuffleNet architecture and its key innovations.","ShuffleNet is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Microsoft|NVIDIA","architecture|deep learning","39"
"1957c1d798","What is question answering and how is it implemented?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Reddit|JPMorgan|Lyft","NLP|language understanding","32"
"74d942f19e","Explain KNN and its computational complexity.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","LinkedIn|Apple|Twitter","ML fundamentals|algorithms","41"
"ef48d6131c","Explain instruction tuning for LLMs.","Fine-tunes LLM on (instruction, response) pairs. Datasets: FLAN, Alpaca. Dramatically improves zero-shot task performance and user intent alignment. Foundation for ChatGPT-like systems (SFT stage). Variants: CoT tuning, multi-turn conversation tuning.","NLP","hard","Google|Apple|OpenAI","instruction tuning|LLM","30"
"1d4f722f53","Describe the Perceiver IO architecture and its key innovations.","Perceiver IO is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","OpenAI|Spotify","architecture|deep learning","40"
"4b8fce22e4","How would you design a news feed ranking system?","Designing a news feed ranking system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Instacart|Twitter","system design|production","42"
"9d158514ef","Explain row-level security in SQL.","Understanding row-level security is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","hard","Doordash|Salesforce|Meta","SQL|database","31"
"0c37456147","Explain virtual environments and dependency management in Python.","Understanding virtual environments and dependency management is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Palantir|OpenAI|JPMorgan","Python|practical skills","31"
"096c203619","Explain label noise handling and its importance in machine learning.","Label Noise Handling is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","medium","LinkedIn|Apple","machine learning|practical ML","50"
"e062333a24","How does RLHF (Reinforcement Learning from Human Feedback) work?","RLHF aligns LLMs with human preferences in three stages: (1) Supervised Fine-Tuning (SFT) on demonstration data. (2) Train a reward model on human comparisons of model outputs (pairwise preferences). (3) Optimize the policy using PPO (Proximal Policy Optimization) against the reward model, with a KL penalty to stay close to the SFT model. Challenges: reward hacking, distribution shift, scalable oversight. Alternatives: DPO (Direct Preference Optimization) skips the reward model entirely.","NLP","hard","Twitter|Doordash","alignment|reinforcement learning|language models","70"
"0604ff8de8","Explain the MERGE/UPSERT operation.","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","Databricks|JPMorgan","SQL|analytics","27"
"984ebb9c19","Explain ablation studies and its importance in machine learning.","Ablation Studies is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","hard","Meta|OpenAI|Instacart","machine learning|practical ML","49"
"5d21486b40","Explain attention mechanisms beyond self-attention in deep learning.","Beyond vanilla self-attention: (1) Cross-attention: queries from one sequence, keys/values from another (decoder attending to encoder). (2) Linear attention: approximate softmax attention in O(n). (3) Sparse attention: attend to fixed patterns (Longformer, BigBird). (4) Multi-query attention: shared keys/values across heads (faster inference). (5) Grouped-query attention (GQA): compromise between MHA and MQA. (6) Flash Attention: IO-aware exact attention (no approximation, 2-4x speedup). (7) Sliding window (Mistral). Choice depends on sequence length and compute budget.","Deep Learning","medium","NVIDIA|Citadel","attention|transformers|efficiency","73"
"8b7733f64b","How would you deploy a machine learning model in production?","Key steps: (1) Model serialization (ONNX, TorchScript, SavedModel). (2) Serving: REST API (Flask/FastAPI), gRPC, or managed services (SageMaker, Vertex AI). (3) Containerization with Docker. (4) Orchestration with Kubernetes. (5) Monitoring: data drift, model performance, latency, errors. (6) CI/CD pipeline for model updates. (7) A/B testing for safe rollout. Consider: batch vs. real-time, latency requirements, scaling, versioning, rollback strategy.","System Design","easy","NVIDIA|Uber|Walmart","deployment|MLOps|production","58"
"ac893d754d","Explain multiprocessing vs threading for ML in Python.","Understanding multiprocessing vs threading for ML is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","easy","Microsoft|Bloomberg|Tesla","Python|practical skills","31"
"0b348cf63a","How do you profile and optimize Python code for ML?","Profiling tools: (1) cProfile: function-level CPU profiling. (2) line_profiler: line-by-line timing. (3) memory_profiler: memory usage per line. (4) py-spy: sampling profiler. (5) torch.profiler for PyTorch. Optimization: (1) Vectorize with NumPy. (2) Use efficient data structures. (3) Caching (@lru_cache). (4) Parallel processing (multiprocessing). (5) Cython/Numba for hot loops. (6) Efficient I/O (Parquet > CSV). Always profile first.","Python","medium","Meta|Salesforce","profiling|optimization|performance","56"
"3ff642c28e","When and how do you use the paired t-test?","The paired t-test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","hard","ByteDance|Citadel","statistical tests|hypothesis testing","44"
"cea0f4210b","Explain the difference between WHERE and HAVING.","WHERE filters rows before aggregation (applied to individual rows). HAVING filters groups after aggregation (applied to aggregated results). Example: 'SELECT dept, AVG(salary) FROM emp WHERE status = active GROUP BY dept HAVING AVG(salary) > 50000' -- WHERE filters individual employees, HAVING filters departments. You cannot use aggregate functions in WHERE (use HAVING instead).","SQL","easy","Reddit|Salesforce","aggregation|filtering","53"
"870a38ebf8","Explain the difference between independent and dependent events.","Two events A and B are independent if P(A and B) = P(A) * P(B), meaning knowledge of one does not affect the probability of the other. Dependent events: P(A and B) = P(A) * P(B|A). Examples: coin flips are independent; drawing cards without replacement is dependent.","Statistics","easy","Uber|Pinterest","probability|independence","47"
"2a0526098c","What is image captioning?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","hard","Meta|Capital One","computer vision|tasks","30"
"6f4f86fe6f","Explain Python's descriptor protocol.","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","hard","Samsung|Apple","Python|advanced","26"
"455f5c7b7e","Explain word sense disambiguation.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Reddit|Google|Apple","NLP|language understanding","32"
"08e2ef0488","Explain label smoothing and its benefits.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","OpenAI|Salesforce","deep learning|architecture","38"
"be8de5e876","Explain Python's datamodel (dunder methods).","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","medium","Meta|Uber|Pinterest","Python|advanced","26"
"7a67ca0dbd","Explain aggregation features (groupby statistics).","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Meta|Spotify","feature engineering|data preprocessing","33"
"c1843e4a00","When and how do you use the two-way ANOVA?","The two-way ANOVA is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","Doordash|Adobe","statistical tests|hypothesis testing","44"
"842cd98643","Explain selection bias in experiments.","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","easy","Snowflake|Two Sigma|NVIDIA","experimentation|methodology","36"
"851712be80","What is the difference between fixed horizon and sequential tests?","This is an important concept in a/b testing that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","A/B Testing","hard","Snowflake|Amazon","methodology|sequential","31"
"bfceb21f08","Design an ML pipeline for training and deploying models at scale.","Components: (1) Data ingestion: scheduled pipelines (Airflow, Dagster) pulling from data warehouse. (2) Feature engineering: feature store with offline and online stores. (3) Training: distributed training (Horovod, PyTorch DDP), experiment tracking (MLflow, W&B), hyperparameter tuning (Optuna, Ray Tune). (4) Evaluation: automated metrics, bias/fairness checks, data validation. (5) Deployment: CI/CD (GitHub Actions), canary/shadow deployment, model registry. (6) Monitoring: data drift (Evidently), model performance, alerting. Orchestration: Kubeflow, Vertex AI Pipelines.","System Design","medium","ByteDance|Microsoft|Stripe|Meta","MLOps|pipeline|infrastructure","68"
"ae2af4ef2d","Explain the architecture of ResNet and why it was revolutionary.","ResNet (He et al., 2015) introduced skip/residual connections: H(x) = F(x) + x, where F(x) is the residual function learned by a few stacked layers. This allowed training of very deep networks (50, 101, 152+ layers) by enabling gradient flow through identity shortcuts. Before ResNet, networks beyond ~20 layers suffered from degradation (not vanishing gradients, but optimization difficulty). ResNet won ILSVRC 2015 and remains foundational in modern architectures.","Computer Vision","medium","Amazon|Jane Street","ResNet|architecture|deep networks","68"
"ce348124a6","How do you handle date/datetime features?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","medium","Doordash|OpenAI","feature engineering|data preprocessing","33"
"2e21d75dd0","Derive the variance of the MLE for a Bernoulli parameter.","For n i.i.d. Bernoulli(p) observations, the MLE is p_hat = X_bar. Var(p_hat) = p(1-p)/n. The Fisher information is I(p) = n / [p(1-p)], so the Cramer-Rao lower bound is 1/I(p) = p(1-p)/n. Since Var(p_hat) equals the CRLB, the MLE is efficient. Asymptotically, sqrt(n)(p_hat - p) -> N(0, p(1-p)).","Statistics","hard","LinkedIn|Pinterest|Bloomberg","estimation|probability|mathematical statistics","48"
"b90f327e5e","Explain experiment tracking and its importance in machine learning.","Experiment Tracking is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","medium","Lyft|Databricks|Uber","machine learning|practical ML","49"
"8f49d7e6ae","What is sample ratio mismatch (SRM) and how do you detect it?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","LinkedIn|Twitter|Huawei","experimentation|methodology","36"
"a91a85837f","Explain the Neyman-Pearson lemma and its implications for hypothesis testing.","The Neyman-Pearson lemma states that for testing simple hypotheses H0: theta=theta0 vs H1: theta=theta1, the most powerful test at significance level alpha is the likelihood ratio test that rejects H0 when L(theta1)/L(theta0) > k, where k is chosen so the Type I error rate equals alpha. This provides the theoretical foundation for likelihood ratio tests and establishes that no other test of the same size can have higher power.","Statistics","hard","ByteDance|Meta","hypothesis testing|mathematical statistics","69"
"ea8f69a402","When and how do you use the independent t-test?","The independent t-test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","Intel|Apple|Meta","statistical tests|hypothesis testing","44"
"c646781b7b","When and how do you use the Granger causality test?","The Granger causality test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","Apple|Amazon|Palantir","statistical tests|hypothesis testing","45"
"03ab89d94d","Explain diffusion models and how they generate images.","Diffusion models define a forward process that gradually adds Gaussian noise to data over T steps: q(x_t|x_{t-1}) = N(x_t; sqrt(1-beta_t)x_{t-1}, beta_t I). The reverse process p_theta(x_{t-1}|x_t) is learned to denoise. Training minimizes a simplified objective: E[||epsilon - epsilon_theta(x_t, t)||^2]. At inference, start from pure noise and iteratively denoise. Score-based formulation connects to score matching and SDEs. DDPM, DDIM, and latent diffusion (Stable Diffusion) are key architectures.","Deep Learning","hard","JPMorgan|LinkedIn|Capital One|Snap","generative models|diffusion|image generation","66"
"f9a05d51cc","What is an A/A test?","Both groups get same experience. Purpose: validate platform (false positive rate matches alpha), verify randomization, check for SRM. If A/A tests show significance, there is a bug. Run periodically.","A/B Testing","easy","Google|Lyft","experimentation|validation","29"
"b1dea78001","Explain materialized views in SQL.","Understanding materialized views is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","OpenAI|Doordash","SQL|database","31"
"3505bd8855","When and how do you use the Hausman test?","The Hausman test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","easy","Meta|Citadel","statistical tests|hypothesis testing","44"
"5162622e23","Explain the Anderson-Darling test and when you would use it.","The Anderson-Darling test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","easy","LinkedIn|Apple|Microsoft","statistical tests|hypothesis testing","63"
"5fb7175469","When would you choose K-Means over DBSCAN and vice versa?","K-Means and DBSCAN are both powerful algorithms but suited to different scenarios. K-Means may be preferred when its assumptions match the data characteristics, while DBSCAN excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","JPMorgan|Google|LinkedIn","model comparison|model selection","59"
"23ea1f27f8","What is triggering analysis in experimentation?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Doordash|Google|Salesforce","experimentation|methodology","36"
"2ae93e87bb","When and how do you use the Jarque-Bera test?","The Jarque-Bera test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","hard","Citadel|JPMorgan","statistical tests|hypothesis testing","44"
"042fcb927e","What is a decision tree and how does it work?","A decision tree recursively splits the feature space using binary decisions to minimize impurity (Gini index or entropy for classification, MSE for regression). Advantages: interpretable, handles non-linear relationships, no feature scaling needed. Disadvantages: prone to overfitting, high variance, greedy splitting. Pruning, ensemble methods (Random Forest, Gradient Boosting) address these limitations.","ML Theory","easy","OpenAI|Airbnb|Doordash|JPMorgan","tree models|classification|regression","50"
"83a1b80dcc","What is prefix tuning?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Amazon|Databricks|NVIDIA","NLP|language understanding","32"
"3d27311aba","How do you manage configuration in Python ML projects?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","medium","Snap|Apple|Meta","Python|advanced","26"
"162cbcf439","Explain concept drift and its importance in machine learning.","Concept Drift is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","easy","Netflix|Capital One|Google","machine learning|practical ML","49"
"8f8a6e905e","Explain underfitting and its importance in machine learning.","Underfitting is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","easy","JPMorgan|NVIDIA|Amazon","machine learning|practical ML","48"
"3e79491bf5","How do you design experiments when you cannot randomize at the user level?","When user-level randomization is impossible (network effects, marketplace dynamics): (1) Cluster randomization -- randomize at geo, market, or community level. Analysis: mixed-effects models, cluster-robust SEs. (2) Switchback experiments -- alternate treatment/control over time periods within units. (3) Diff-in-diff -- compare treatment and control groups before/after. (4) Synthetic control -- create a weighted combination of untreated units as counterfactual. (5) Regression discontinuity -- exploit threshold-based treatment assignment. Each has specific assumptions and power implications.","A/B Testing","hard","Walmart|JPMorgan|LinkedIn","causal inference|quasi-experiments|marketplace experimentation","73"
"6ddff3b21c","What is a neural network?","A neural network is a computational model inspired by biological neurons. It consists of layers of interconnected nodes (neurons) that apply linear transformations followed by non-linear activation functions. The universal approximation theorem guarantees that a sufficiently wide single-hidden-layer network can approximate any continuous function. Modern deep networks stack many layers to learn hierarchical representations.","Deep Learning","easy","Snowflake|Two Sigma","neural networks|fundamentals","54"
"07c3a4009e","How do you analyze experiments with network effects?","Graph cluster randomization, ego-network randomization, measure direct+indirect effects, simulation, switchback designs. Analysis: cluster-robust SEs, hierarchical models. Ignoring effects typically underestimates treatment effect.","A/B Testing","hard","Google|Palantir|Amazon","network effects|causal inference","22"
"28b1f81335","How do you handle novelty effects and seasonality in A/B tests?","Novelty effects: users initially engage more with new features, biasing results. Solutions: (1) Run 2-4 weeks minimum. (2) Analyze only post-novelty users. (3) Segment by new vs. returning users. (4) Look at time trends. Seasonality: (1) Run full business cycles. (2) Same-period comparison. (3) Time-based covariates. (4) Pre-experiment deseasonalizing. (5) CUPED/regression adjustment for variance reduction.","A/B Testing","medium","Spotify|DeepMind","experimentation|temporal effects|methodology","55"
"a1f81c4007","Explain database sharding strategies in SQL.","Understanding database sharding strategies is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Snap|Meta","SQL|database","32"
"acf7badece","Explain hyperparameter tuning in deep learning.","Approaches: (1) Grid search: exhaustive but exponential cost. (2) Random search: better coverage of important dimensions (Bergstra and Bengio, 2012). (3) Bayesian optimization: models objective as GP, uses acquisition function (TPE in Optuna, GP in BoTorch). (4) Hyperband/ASHA: early stopping of poor configurations. (5) Population-based training: evolutionary approach. Tools: Optuna, Ray Tune, W&B Sweeps. Key: define good search space, choose right metric, use enough budget. For deep learning: learning rate and batch size are most impactful.","Deep Learning","medium","Uber|NVIDIA|Samsung","hyperparameters|optimization|AutoML","76"
"0254db321c","Explain the concept of query execution order in SQL.","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","LinkedIn|OpenAI","SQL|analytics","27"
"1df4c8f28d","Explain generators for ML data pipelines.","Functions that yield values lazily. Memory-efficient, support infinite sequences. ML uses: batch loading from disk, streaming augmentation, lazy processing. PyTorch DataLoader uses generator patterns.","Python","medium","Amazon|Walmart|Uber","generators|data loading","24"
"11df65b05e","Explain the difference between intent-to-treat and per-protocol analysis.","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","hard","Walmart|NVIDIA","experimentation|methodology","36"
"1bd26f69c8","What is text classification?","Assigns labels to text. Traditional: TF-IDF + logistic regression. Deep: CNN for text, BiLSTM, BERT fine-tuning. Zero-shot: LLMs, NLI models. Tasks: sentiment, spam, topic classification, intent detection. BERT fine-tuning is the standard baseline.","NLP","easy","Netflix|DeepMind|Two Sigma","text classification|NLP","33"
"d6f48b1c7b","Explain the McNemar's test and when you would use it.","The McNemar's test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","hard","Instacart|OpenAI|Uber","statistical tests|hypothesis testing","63"
"224107f015","What is weight decay and how does it relate to L2 regularization?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Jane Street|Stripe","deep learning|architecture","38"
"a2314754d7","What is the kernel trick in SVMs?","The kernel trick allows SVMs to learn non-linear decision boundaries by implicitly mapping data to a high-dimensional feature space without explicitly computing the transformation. The kernel function K(x_i, x_j) = phi(x_i) . phi(x_j) computes inner products in the transformed space. Common kernels: linear, polynomial, RBF (Gaussian), sigmoid. This makes SVMs computationally tractable for non-linear problems.","ML Theory","medium","Citadel|Goldman Sachs|Databricks|Netflix","SVM|kernel methods|classification","55"
"56569fb3f2","What is an A/B test and how does it work?","An A/B test (randomized controlled experiment) compares two variants (A: control, B: treatment) to measure the causal effect of a change. Steps: (1) Define hypothesis and primary metric. (2) Calculate required sample size (power analysis). (3) Randomly assign users to groups. (4) Run experiment for predetermined duration. (5) Analyze results with statistical tests. (6) Make a decision. Key principles: randomization (eliminates confounders), control group (baseline), single variable (isolate effect).","A/B Testing","easy","Meta|DeepMind|OpenAI|Microsoft","experimentation|causal inference","69"
"e45db7bc93","Describe the RetNet architecture and its key innovations.","RetNet is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Google|Two Sigma|Databricks","architecture|deep learning","39"
"1a82dcbd97","Explain batch size theory in deep learning.","Small batches: noisy gradients (regularization), better generalization. Large batches: accurate gradients, faster but may converge to sharp minima. Linear scaling rule: scale LR with batch size. Critical batch size gives diminishing returns beyond it.","Deep Learning","hard","Palantir|NVIDIA","batch size|optimization","34"
"58b736f1ab","When would you choose AdaBoost over Gradient Boosting and vice versa?","AdaBoost and Gradient Boosting are both powerful algorithms but suited to different scenarios. AdaBoost may be preferred when its assumptions match the data characteristics, while Gradient Boosting excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Walmart|OpenAI|Google","model comparison|model selection","61"
"f7637fc993","Explain Neural Architecture Search (NAS) approaches.","NAS automates neural network design. Approaches: (1) Reinforcement learning-based: controller generates architectures, reward is validation accuracy (Zoph and Le, 2017). (2) Evolutionary: mutate/crossover architecture genes. (3) Differentiable (DARTS): relax discrete architecture choices to continuous, optimize with gradient descent. (4) One-shot / supernet: train a single large network, sample sub-networks. Trade-offs: compute cost, search space design, proxy tasks. Modern NAS is >1000x cheaper than original approaches.","Deep Learning","hard","Two Sigma|DeepMind|Airbnb","architecture search|AutoML|optimization","65"
"8ba7a4787a","Explain model calibration.","Well-calibrated model: predicted probabilities match actual frequencies. Assessment: reliability diagrams, Brier score, ECE. Calibration methods: Platt scaling, isotonic regression, temperature scaling. Important for medical diagnosis, risk assessment.","ML Theory","medium","Netflix|DeepMind|Airbnb","calibration|probability|evaluation","27"
"c3a98c0da7","What is regularization? Compare L1 and L2.","Regularization adds a penalty term to the loss function to constrain model complexity. L1 (Lasso): penalty = lambda * sum|w_i|; produces sparse solutions (feature selection). L2 (Ridge): penalty = lambda * sum(w_i^2); shrinks weights toward zero but rarely sets them exactly to zero. Elastic Net combines both. The regularization strength lambda controls the bias-variance tradeoff.","ML Theory","easy","Bloomberg|Snowflake","regularization|feature selection","55"
"82bcadd34a","Explain the concept of sufficient statistics and the Rao-Blackwell theorem.","A sufficient statistic T(X) captures all information in the data about parameter theta: P(X|T,theta) does not depend on theta (Factorization theorem). The Rao-Blackwell theorem states that conditioning any unbiased estimator on a sufficient statistic yields a new estimator with equal or lower variance. Combined with completeness (Lehmann-Scheffe), this gives the unique minimum variance unbiased estimator (UMVUE).","Statistics","hard","Microsoft|Walmart","mathematical statistics|estimation|information theory","56"
"005376695d","Explain probability density functions.","Probability Density Functions is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Apple|Amazon","probability|distributions","36"
"11d6ad85ca","What is the difference between extractive and abstractive summarization?","Extractive summarization selects and concatenates the most important sentences/phrases from the source text. Methods: TextRank, sentence scoring, BERT-based selection. Abstractive summarization generates new text that captures the key information, potentially using words not in the source. Methods: seq2seq models, Transformers (BART, T5, Pegasus). Extractive is more faithful but less fluent; abstractive is more fluent but may hallucinate. Modern systems often combine both.","NLP","medium","Apple|Tesla","summarization|text generation","62"
"e5cd3e5f2e","What are challenges in multilingual NLP?","Data scarcity for low-resource languages, script diversity, morphological complexity, code-switching, cultural nuances, tokenization quality. Approaches: mBERT, XLM-R, cross-lingual transfer, translation-based methods.","NLP","hard","Snap|Snowflake|Uber","multilingual|cross-lingual","21"
"45d2b1963d","What is the difference between pointwise, pairwise, and listwise ranking?","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Microsoft|NVIDIA","ranking|learning to rank","31"
"180a4baf8f","What is the difference between UNION and UNION ALL?","UNION combines results from multiple SELECT statements and removes duplicates (performs a DISTINCT). UNION ALL combines results without removing duplicates. UNION ALL is faster because it skips the deduplication step. Use UNION when you need unique results; use UNION ALL when duplicates are acceptable or impossible (e.g., combining disjoint datasets). Both require matching column counts and compatible data types.","SQL","medium","Tesla|Spotify|Amazon","set operations|SQL basics","59"
"efb21f244c","How do you select the right evaluation metric for your ML problem?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","easy","Databricks|Uber","ML fundamentals|algorithms","41"
"c47d3d972e","What is image generation with GANs vs diffusion models?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","hard","Apple|Meta","computer vision|tasks","30"
"e51df10750","What is cross-validation and why is it important?","Cross-validation is a resampling technique for evaluating model performance. K-fold CV splits data into K folds, trains on K-1, validates on 1, and rotates. It provides a more reliable estimate of out-of-sample performance than a single train/test split. Variants include stratified K-fold, leave-one-out, and time-series CV. It helps detect overfitting and is used for hyperparameter tuning.","ML Theory","easy","Google|Netflix","model evaluation|validation","56"
"3bdf7319f2","Explain error handling in ML pipelines in Python.","Understanding error handling in ML pipelines is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Adobe|Microsoft|Amazon","Python|practical skills","31"
"64f35c995e","Describe the U-Net architecture and its key innovations.","U-Net is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Amazon|Meta","architecture|deep learning","39"
"dc0ac8bb1c","What is an interleaving experiment?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","LinkedIn|Stripe","experimentation|methodology","36"
"b483e0d6da","Explain the concept of standard deviation.","Standard deviation measures the average amount of variability in a dataset -- how spread out the data points are from the mean. It is the square root of the variance. A low SD means data points are close to the mean; a high SD means they are spread out. In a normal distribution, approximately 68% of data falls within 1 SD, 95% within 2 SDs, and 99.7% within 3 SDs of the mean.","Statistics","easy","Google|Doordash|Apple","descriptive statistics|variability","73"
"fe794f4da3","Explain the difference between frequentist and Bayesian A/B testing.","Frequentist: fixed sample size, compute p-value, reject H0 if p < alpha. Reports confidence intervals. Cannot say 'probability that B is better.' Bayesian: uses prior + data to compute posterior distribution of the treatment effect. Can directly compute P(B > A | data). Allows continuous monitoring without peeking problems. Bayesian decisions: expected loss, probability of being best. Frequentist is simpler, well-understood; Bayesian is more flexible, provides richer conclusions but requires prior specification.","A/B Testing","medium","Capital One|Uber|Meta|Amazon","bayesian statistics|hypothesis testing|methodology","72"
"632d9b0409","What is batch normalization?","Batch normalization normalizes the input of each layer to have zero mean and unit variance across the mini-batch, then applies a learned affine transformation. Benefits: (1) enables higher learning rates, (2) reduces internal covariate shift, (3) has slight regularization effect, (4) makes training less sensitive to initialization. Applied before or after the activation function. Layer normalization is preferred for sequence models.","Deep Learning","easy","Snowflake|NVIDIA","normalization|neural networks|training","61"
"bc23d1a721","What is the expectation-maximization (EM) algorithm? Derive the E and M steps for a Gaussian Mixture Model.","EM is an iterative algorithm for MLE with latent variables. E-step: compute posterior probabilities (responsibilities) of each data point belonging to each cluster given current parameters. M-step: update parameters (means, covariances, mixing weights) using weighted MLE with responsibilities as weights. EM is guaranteed to increase the log-likelihood (or keep it constant) at each iteration and converges to a local maximum. For GMM: E-step computes gamma_nk = pi_k N(x_n|mu_k,Sigma_k) / sum_j pi_j N(x_n|mu_j,Sigma_j); M-step updates mu_k, Sigma_k, pi_k.","Statistics","hard","Meta|Apple|Google|Uber","estimation|clustering|latent variables","77"
"a4f125402b","How do you implement a funnel analysis in SQL?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Jane Street|Meta","SQL|analytics","27"
"678e6f4427","What is the information gain criterion in decision trees?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Twitter|Microsoft|DeepMind","ML fundamentals|algorithms","41"
"27c52134bf","What is a p-value?","A p-value is the probability of observing a test statistic at least as extreme as the one computed from the sample data, assuming the null hypothesis is true. It is NOT the probability that the null hypothesis is true. A small p-value (typically < 0.05) suggests the observed data is unlikely under H0, leading us to reject H0. Common misconceptions include treating p-values as effect sizes or as the probability of making an error.","Statistics","easy","Databricks|Google","hypothesis testing|inference","74"
"c77d03b823","How do you handle cyclical features (hour, day of week)?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","medium","Amazon|NVIDIA","feature engineering|data preprocessing","33"
"4ee93939af","Describe the Stable Diffusion architecture and its key innovations.","Stable Diffusion is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Meta|Google","architecture|deep learning","40"
"9c4c0281ac","What is TF-IDF and when would you use it?","TF-IDF (Term Frequency - Inverse Document Frequency) weighs terms by how important they are to a document relative to a corpus. TF = count of term in doc / total terms in doc. IDF = log(total docs / docs containing term). TF-IDF = TF * IDF. High TF-IDF means the term is frequent in the document but rare overall. Use for: text classification, information retrieval, keyword extraction. Limitations: ignores word order and semantics.","NLP","easy","Amazon|Google","text representation|information retrieval","73"
"ab1f2326d9","Explain feature interactions and polynomial features.","Feature interactions capture combined effects: x1*x2 (multiplication), x1/x2 (ratio), x1-x2 (difference). Polynomial features: x, x^2, x^3 for non-linear relationships (PolynomialFeatures in sklearn). Risks: exponential growth in feature space (n features, degree d: O(n^d)), overfitting, multicollinearity. Mitigation: regularization, feature selection (L1), tree-based models (automatically learn interactions). Domain knowledge should guide which interactions to create. Modern approach: deep learning learns interactions implicitly.","Feature Engineering","medium","Netflix|Capital One","feature interactions|polynomial features|non-linearity","60"
"4864f4de19","Explain domain-specific feature engineering for fraud detection.","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Amazon|Twitter|Samsung","feature engineering|data preprocessing","33"
"4cf1632f8a","What is feature hashing (hashing trick)?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","hard","Amazon|Tesla|LinkedIn","feature engineering|data preprocessing","33"
"955cf2ea17","What is gradient boosting? Compare XGBoost, LightGBM, and CatBoost.","Gradient boosting sequentially fits weak learners (trees) to the negative gradient of the loss function. Each tree corrects errors of the ensemble so far. XGBoost: regularized objective, column subsampling, efficient handling of sparse data. LightGBM: leaf-wise growth, histogram-based splitting, faster on large data. CatBoost: ordered boosting, native categorical feature handling, less overfitting. All three dominate tabular data competitions.","ML Theory","medium","Amazon|Databricks|NVIDIA","ensemble methods|boosting|tree models","58"
"b554ed2233","How do you handle features with many zeros (sparse features)?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Netflix|Meta|Citadel","feature engineering|data preprocessing","33"
"00707772d4","How do you deduplicate data in SQL?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Amazon|Uber|Salesforce","SQL|analytics","27"
"b57ffb7365","Explain precision, recall, and F1-score.","Precision = TP/(TP+FP): of all positive predictions, how many are correct. Recall = TP/(TP+FN): of all actual positives, how many did we find. F1 = 2*P*R/(P+R): harmonic mean of precision and recall. Use precision when false positives are costly (spam detection); use recall when false negatives are costly (disease screening). F1 balances both. The PR curve and AUC-PR are especially useful for imbalanced datasets.","ML Theory","easy","OpenAI|Intel","classification metrics|evaluation","64"
"89ff462267","Explain the difference between parametric and non-parametric tests.","Parametric tests (e.g., t-test, ANOVA) assume the data follows a specific distribution (usually normal) and use parameters like mean and variance. Non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) make fewer assumptions about the data distribution and are based on ranks or signs. Non-parametric tests are more robust to outliers and skewed data but generally have less statistical power when parametric assumptions hold.","Statistics","medium","Google|Databricks","hypothesis testing|statistical tests","61"
"229f26dbc1","What is a self join and when would you use one?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Snowflake|Tesla|Microsoft","SQL|analytics","27"
"0cfe1f9728","What is target encoding and how do you avoid overfitting with it?","Target encoding replaces each category with the mean (or other statistic) of the target variable for that category. Risk: leaking target information causes overfitting. Prevention: (1) K-fold target encoding: compute encodings using out-of-fold target values. (2) Smoothing: blend category mean with global mean weighted by category size: encoding = (count * cat_mean + m * global_mean) / (count + m). (3) Add Gaussian noise. (4) Leave-one-out encoding. CatBoost uses ordered target encoding to prevent leakage.","Feature Engineering","medium","OpenAI|Apple|Two Sigma","target encoding|categorical features|leakage","75"
"7ef5db146f","Explain the PAC learning framework and VC dimension.","PAC (Probably Approximately Correct) learning formalizes learnability: a concept class is PAC-learnable if an algorithm can, with probability >= 1-delta, produce a hypothesis with error <= epsilon, in polynomial time/samples. VC dimension measures the capacity of a hypothesis class -- the largest set of points it can shatter. VC dimension determines sample complexity: m >= O((d*log(1/epsilon) + log(1/delta))/epsilon) for PAC learning with VC dimension d.","ML Theory","hard","Twitter|Apple|Meta|Uber","learning theory|generalization|computational complexity","65"
"12c4704ee1","Explain normalization techniques comparison in deep learning.","Layer Norm: normalizes across features within a single sample (standard in Transformers). Batch Norm: normalizes across the batch for each feature (standard in CNNs). Instance Norm: per-sample, per-channel (style transfer). Group Norm: divides channels into groups, normalizes within groups (small batch sizes). RMS Norm: simplified Layer Norm without mean centering (used in LLaMA). Key differences: Batch Norm depends on batch statistics (issues with small batches, different train/test behavior); Layer Norm is batch-independent.","Deep Learning","medium","Meta|Amazon","normalization|architecture|training","72"
"174e73c4a1","What is duck typing in Python?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","hard","Microsoft|Google|Meta","Python|advanced","26"
"917b0b8dd9","Explain interference in experiments and methods to handle it.","Interference (SUTVA violation) occurs when one unit's treatment affects another unit's outcome. Examples: social networks (treated user's activity affects friends), marketplaces (price changes affect competitors), shared resources. Detection: test for spillovers by comparing control units near/far from treated units. Solutions: (1) Cluster randomization (randomize connected groups). (2) Ego-cluster randomization. (3) Graph cluster randomization (partition social graph). (4) Two-stage randomization (randomize clusters, then individuals within). (5) Model interference explicitly. Ignoring interference biases treatment effect estimates.","A/B Testing","hard","LinkedIn|Capital One|Amazon","interference|network effects|causal inference","74"
"595d10040b","How do you determine the sample size for an A/B test?","Sample size depends on: (1) Significance level (alpha, typically 0.05). (2) Power (1-beta, typically 0.80). (3) Minimum detectable effect (MDE) -- smallest meaningful difference. (4) Baseline metric variance. Formula for proportions: n = (Z_{alpha/2} + Z_beta)^2 * (p1*(1-p1) + p2*(1-p2)) / (p1-p2)^2. Practical considerations: use online calculators, account for multiple comparisons, consider one-tailed vs. two-tailed tests. Larger effects need smaller samples; smaller alpha/higher power need larger samples.","A/B Testing","easy","Samsung|Meta","power analysis|sample size|statistics","67"
"4b19aef649","What is the multi-armed bandit approach and how does it compare to A/B testing?","Multi-armed bandits (MAB) balance exploration and exploitation. Algorithms: epsilon-greedy, UCB, Thompson Sampling. Compared to A/B testing: MABs adaptively allocate traffic to better-performing variants, reducing regret. Trade-off: A/B tests provide cleaner statistical inference; MABs optimize cumulative reward. Use MABs for continuous optimization and many variants. Use A/B tests for statistical rigor and causal claims.","A/B Testing","medium","Two Sigma|Samsung","bandits|experimentation|optimization","53"
"749ac9c255","Explain entity embeddings for categorical variables and its importance in machine learning.","Entity Embeddings For Categorical Variables is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","medium","Intel|Twitter|Uber","machine learning|practical ML","52"
"adf18f99c4","What is the difference between one-tailed and two-tailed tests?","A two-tailed test checks for any difference: H1: mu != mu0. A one-tailed test checks a specific direction: H1: mu > mu0 or H1: mu < mu0. One-tailed tests have more power in the specified direction but miss effects in the opposite direction. Use two-tailed unless you have strong prior reason.","Statistics","easy","Meta|Amazon","hypothesis testing|inference","51"
"3cffcbce7c","When would you choose Linear Regression over Decision Tree and vice versa?","Linear Regression and Decision Tree are both powerful algorithms but suited to different scenarios. Linear Regression may be preferred when its assumptions match the data characteristics, while Decision Tree excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Bloomberg|Adobe","model comparison|model selection","63"
"57edb9831c","What is a Siamese network and when is it used?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","easy","Apple|Google","deep learning|architecture","38"
"49090b045b","What is a memory network?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Walmart|JPMorgan","deep learning|architecture","38"
"c5eca92dca","How would you design a text-to-speech system?","Designing a text-to-speech system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Netflix|Walmart|Google|DeepMind","system design|production","40"
"781e8180f7","What is hierarchical clustering?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Palantir|Uber","ML fundamentals|algorithms","41"
"a13a3a4c75","What is a subquery?","A query nested inside another. Types: scalar (single value), row, table, correlated (references outer query). Use: filtering by aggregates, EXISTS checks, derived tables. CTEs often preferred for readability.","SQL","easy","Google|DeepMind|OpenAI","subqueries|SQL basics","28"
"785031eae6","What is power transformation and when to use it?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Snowflake|Apple|Microsoft","feature engineering|data preprocessing","33"
"d6652895c2","When would you choose Naive Bayes over Logistic Regression and vice versa?","Naive Bayes and Logistic Regression are both powerful algorithms but suited to different scenarios. Naive Bayes may be preferred when its assumptions match the data characteristics, while Logistic Regression excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Twitter|Microsoft","model comparison|model selection","63"
"ae4ed22bc1","Explain mean shift clustering.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","DeepMind|Reddit|Microsoft","ML fundamentals|algorithms","41"
"e4b91eb4e1","Explain Retrieval-Augmented Generation (RAG).","RAG combines a retriever (finds relevant documents from a knowledge base) with a generator (LLM that conditions on retrieved context). Pipeline: encode query, retrieve top-k documents via dense retrieval (e.g., DPR, Contriever) or sparse retrieval (BM25), concatenate with the query, and feed to the generator. Benefits: reduces hallucination, enables knowledge updates without retraining, grounds generation in evidence. Challenges: retrieval quality, latency, context window limits.","NLP","medium","Adobe|Google|Robinhood|Spotify","retrieval|language models|knowledge-grounded","64"
"6cf122f269","What is optical flow?","Estimates pixel motion between frames. Methods: Lucas-Kanade, Horn-Schunck, FlowNet, RAFT. Applications: video stabilization, action recognition, autonomous driving, video interpolation. Challenges: occlusions, large displacements, lighting.","Computer Vision","medium","Capital One|Samsung|Amazon","optical flow|video","24"
"1ec1c42f57","Explain CTEs (Common Table Expressions) and recursive CTEs.","A CTE (WITH clause) defines a temporary named result set for the duration of a query. Benefits: readability, reusability, modularity. Recursive CTEs reference themselves: WITH RECURSIVE cte AS (base_case UNION ALL SELECT ... FROM cte WHERE condition). Use cases: hierarchical data (org charts, category trees), graph traversal, generating series. Performance note: CTEs may or may not be materialized depending on the database engine.","SQL","medium","Adobe|NVIDIA|Bloomberg","CTE|recursive queries|SQL advanced","63"
"6a497c4c29","What is continuous training in ML systems?","This is an important concept in system design that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","System Design","medium","Amazon|Google|Microsoft","MLOps|automation","31"
"c1e07c7f4c","What is the Lottery Ticket Hypothesis?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","easy","Amazon|Snap","deep learning|architecture","38"
"734788e34d","What are support vectors in SVM?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","easy","Jane Street|Lyft|Amazon","ML fundamentals|algorithms","41"
"e1b7f0b317","What is ensemble learning?","Combines multiple models. Bagging: parallel, reduces variance (Random Forest). Boosting: sequential, reduces bias (XGBoost). Stacking: meta-model on base predictions. Voting: majority/average. Ensembles almost always outperform individual models.","ML Theory","medium","Amazon|Google|Robinhood","ensemble methods|model selection","27"
"58695bc663","Explain model monitoring in production and its importance in machine learning.","Model Monitoring In Production is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","hard","Bloomberg|JPMorgan|Walmart","machine learning|practical ML","51"
"a836166c8c","How do you create time-based features for a machine learning model?","Time decomposition: year, month, day, day_of_week, hour, is_weekend, is_holiday, quarter, week_of_year. Cyclical encoding: sin/cos transforms for cyclical features (hour, month). Lag features: value at t-1, t-7, etc. Rolling statistics: rolling mean, std, min, max over windows. Time since events: days since last purchase, time to next event. Trend features: linear trend, exponential smoothing. Interaction: time * category. Domain-specific: business hours, seasonal indicators.","Feature Engineering","medium","Adobe|Meta|Capital One|Tesla","time features|temporal data|feature engineering","62"
"2807d1f2a7","Explain data drift detection and its importance in machine learning.","Data Drift Detection is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","hard","Google|Amazon","machine learning|practical ML","50"
"6d9812bff3","When would you choose Elastic Net over Lasso and vice versa?","Elastic Net and Lasso are both powerful algorithms but suited to different scenarios. Elastic Net may be preferred when its assumptions match the data characteristics, while Lasso excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Microsoft|Stripe","model comparison|model selection","61"
"3630c19fd9","Write a SQL query to find the second highest salary in each department.","Using window functions: WITH ranked AS (SELECT *, DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rnk FROM employees) SELECT department, employee_name, salary FROM ranked WHERE rnk = 2. Alternative using correlated subquery: SELECT e1.* FROM employees e1 WHERE 1 = (SELECT COUNT(DISTINCT salary) FROM employees e2 WHERE e2.department = e1.department AND e2.salary > e1.salary). DENSE_RANK handles ties correctly (vs. ROW_NUMBER which would be arbitrary).","SQL","medium","Adobe|Google|Meta|Amazon","window functions|subqueries|ranking","67"
"ff72cf0f68","Explain model parallelism in deep learning.","Model parallelism splits a model across multiple GPUs. Pipeline parallelism: different layers on different GPUs (micro-batching for efficiency, GPipe, PipeDream). Tensor parallelism: splits individual operations across GPUs (Megatron-LM for attention/FFN). Data parallelism: same model on all GPUs, different data (DDP, FSDP/ZeRO). Combinations: 3D parallelism (data + pipeline + tensor) for training very large models (GPT-3, PaLM). Key challenges: communication overhead, memory balance, bubble time.","Deep Learning","medium","Databricks|Snowflake|OpenAI","distributed training|scaling|infrastructure","64"
"1434bdb816","Explain indexes and how they improve performance.","B-tree (default, range queries), hash (equality), GIN (full-text, JSON), bitmap (low cardinality), composite (multi-column). Index WHERE/JOIN/ORDER BY columns. Use EXPLAIN to verify usage.","SQL","medium","Uber|Apple|Twitter","indexing|performance","23"
"db7fcfe05a","What is data versioning?","Tracks dataset changes over time. Tools: DVC, Delta Lake, LakeFS. Importance: reproducibility, debugging, compliance, collaboration. Version training data, validation data, and transformations together.","Feature Engineering","medium","Google|Uber","data versioning|MLOps","23"
"3025b6a547","What is a feature store and why would you use one?","A feature store is a centralized repository for storing, managing, and serving ML features. Benefits: (1) feature reuse across teams/models, (2) consistency between training and serving (avoids train-serve skew), (3) point-in-time correctness for historical features, (4) feature versioning and lineage, (5) low-latency online serving. Examples: Feast, Tecton, Hopsworks, AWS Feature Store. Architecture: offline store (batch, historical) + online store (real-time, low-latency).","System Design","easy","Stripe|Walmart","feature engineering|MLOps|infrastructure","61"
"d0f3722b9c","What is the difference between filter and wrapper feature selection?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Google|Robinhood","feature engineering|data preprocessing","33"
"a459101843","Design an anomaly detection system for monitoring microservices.","Architecture: (1) Data collection: metrics (CPU, memory, latency, error rates) via Prometheus, logs via ELK, traces via Jaeger. (2) Feature engineering: rolling statistics, rate of change, day-over-day comparison. (3) Models: statistical (z-score, IQR), ML (Isolation Forest, LSTM autoencoder for time series), ensemble. (4) Alerting: severity levels, deduplication, routing (PagerDuty). (5) Root cause analysis: correlation with deployments, dependency graph analysis.","System Design","hard","Lyft|Google","anomaly detection|monitoring|system design","59"
"92aa60b67a","Explain the architecture of a modern NLP pipeline for production.","Components: (1) Text preprocessing: cleaning, normalization, language detection. (2) Tokenization: subword (BPE/WordPiece). (3) Model: fine-tuned Transformer or LLM API. (4) Post-processing: decoding, formatting, filtering. Infrastructure: (5) Feature store for user/document embeddings. (6) Model serving: ONNX Runtime, TensorRT, vLLM for LLMs. (7) Monitoring: input/output logging, drift detection, latency tracking. (8) Evaluation: automated metrics + human evaluation. (9) Feedback loop: active learning, RLHF.","NLP","hard","Google|ByteDance","production|MLOps|infrastructure","61"
"b03a803d30","What is the vanishing gradient problem?","Gradients become exponentially small in deep networks, making early layers train slowly. Caused by sigmoid/tanh activations. Solutions: ReLU, residual connections, batch/layer norm, careful initialization (Xavier, He), LSTM/GRU.","Deep Learning","easy","Snap|Amazon","training|optimization|gradients","27"
"04c5e3ad10","What is feature importance?","Measures feature contribution. Methods: tree-based (Gini), permutation (model-agnostic), SHAP (game-theoretic), coefficients (linear models), drop-column (expensive). Use permutation or SHAP for reliable results.","Feature Engineering","easy","NVIDIA|DeepMind|LinkedIn","feature importance|interpretability","22"
"1cf9271932","What is the difference between object detection and image segmentation?","Object detection locates and classifies objects with bounding boxes (YOLO, SSD, Faster R-CNN, DETR). Image segmentation assigns a class to each pixel. Semantic segmentation labels every pixel by class (FCN, U-Net, DeepLab). Instance segmentation distinguishes individual objects of the same class (Mask R-CNN). Panoptic segmentation combines both (stuff + things). Detection is faster; segmentation provides finer spatial understanding.","Computer Vision","easy","Microsoft|Netflix|Uber","object detection|segmentation|tasks","58"
"43daad6ab5","How do you select primary and guardrail metrics for an experiment?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","easy","Airbnb|Uber|Databricks","experimentation|methodology","36"
"d4522c236d","What is depth estimation from images?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","Apple|Databricks|Twitter","computer vision|tasks","30"
"ed64b916e9","What is group normalization and when is it preferred over batch norm?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","DeepMind|Microsoft","deep learning|architecture","38"
"4fc7e66773","Explain the chi-squared test and when you would use it.","The chi-squared test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Instacart|Airbnb|ByteDance","statistical tests|hypothesis testing","63"
"31a665dcbc","Explain NULL handling in aggregations in SQL.","Understanding NULL handling in aggregations is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Intel|Two Sigma|Stripe","SQL|database","33"
"8b5033354a","Explain the concept of neural scaling laws.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Adobe|ByteDance|Twitter","deep learning|architecture","38"
"9294abfca7","What is anomaly detection and what approaches exist?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","easy","Stripe|Goldman Sachs","ML fundamentals|algorithms","41"
"5ca5cbcca8","Explain the difference between hard and soft clustering.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","JPMorgan|Snowflake","ML fundamentals|algorithms","41"
"8fb20314c9","Explain temporal action detection.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","hard","Huawei|DeepMind|Uber","computer vision|tasks","30"
"23468ded99","Explain the difference between L1 and L2 loss functions.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Google|LinkedIn|Databricks","ML fundamentals|algorithms","41"
"395e54e410","What is recall at k and how is it used for ranking?","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Apple|Instacart","ranking|evaluation","31"
"ff54ab0916","What is multimodal learning?","This is an important concept in deep learning that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","Deep Learning","hard","Walmart|Google|Salesforce","multimodal|architecture","31"
"f2d31444bf","Explain zero-shot learning approaches.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Meta|Palantir","deep learning|architecture","38"
"3a74a52d50","Describe the Mamba (state space model) architecture and its key innovations.","Mamba (state space model) is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Adobe|Snowflake|Jane Street","architecture|deep learning","42"
"5aab34ec14","Derive the closed-form solution for linear regression and explain its computational complexity.","Minimize L(w) = ||Xw - y||^2. Taking the gradient and setting to zero: dL/dw = 2X^T(Xw-y) = 0, so w* = (X^T X)^{-1} X^T y (the normal equation). Computational complexity: O(n*p^2) for X^T X, O(p^3) for the inverse -- prohibitive when p is large. Alternatives: QR decomposition (numerically stable), SVD, gradient descent (scalable), or regularized forms (Ridge: w* = (X^T X + lambda I)^{-1} X^T y).","ML Theory","hard","JPMorgan|Google","linear models|optimization|computational complexity","66"
"0a47590d96","Explain meta-learning.","Models that quickly adapt to new tasks with few examples. Metric-based: Prototypical Networks. Optimization-based: MAML (fast fine-tuning). Model-based: hypernetworks. Applications: few-shot classification, drug discovery, robotics.","ML Theory","hard","Spotify|Databricks","meta-learning|few-shot","25"
"741ab91c3e","When would you choose LightGBM over CatBoost and vice versa?","LightGBM and CatBoost are both powerful algorithms but suited to different scenarios. LightGBM may be preferred when its assumptions match the data characteristics, while CatBoost excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","DeepMind|Airbnb","model comparison|model selection","59"
"5fc281a6e6","Explain the concept of language model fine-tuning with LoRA.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Google|Pinterest","NLP|language understanding","32"
"2982414c56","Explain stratified sampling and its importance in machine learning.","Stratified Sampling is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","medium","Apple|Tesla|DeepMind","machine learning|practical ML","49"
"b73d12a2a4","How would you design a video recommendation system?","Designing a video recommendation system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","NVIDIA|Snap","system design|production","41"
"88612418c0","How would you design a ride-sharing pricing algorithm?","Designing a ride-sharing pricing algorithm requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","Google|Meta|OpenAI|Snowflake","system design|production","41"
"70afd3df34","What is tokenization in NLP?","Tokenization splits text into smaller units (tokens). Types: word-level (splits on whitespace/punctuation), subword (BPE, WordPiece, SentencePiece -- handle OOV words by breaking rare words into common subunits), character-level (individual characters). Subword tokenization is standard in modern models (BERT uses WordPiece, GPT uses BPE). Tokenization affects vocabulary size, sequence length, and the ability to handle morphologically rich languages.","NLP","easy","OpenAI|Citadel|Palantir","text preprocessing|tokenization","57"
"9d7c538b65","How would you approach feature engineering for a recommendation system?","User features: demographics, tenure, activity level, preference history (avg rating, genre distribution), engagement metrics. Item features: metadata (category, price, brand), popularity (views, purchases, ratings), recency, text embeddings (title, description). Interaction features: user-item affinity (past ratings), collaborative signals (users-who-bought-also-bought), time since last interaction. Context: time of day, device, session depth. Advanced: graph features (user-item bipartite graph centrality), sequence features (last N items viewed/purchased), diversity metrics. Feature store manages real-time and batch features.","Feature Engineering","hard","Google|Reddit","recommendation systems|feature engineering|user modeling","71"
"520c63abb2","What is the difference between INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN?","INNER JOIN returns rows with matching keys in both tables. LEFT JOIN returns all rows from the left table and matched rows from the right (NULL for non-matches). RIGHT JOIN is the reverse. FULL OUTER JOIN returns all rows from both tables (NULLs where no match). Choice depends on whether you need all records from one/both tables or only matches. Performance: INNER JOINs are typically fastest; consider indexing join columns.","SQL","easy","Meta|Amazon|Pinterest","joins|SQL basics","70"
"36d88cd7d0","Describe the MobileNet architecture and its key innovations.","MobileNet is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","LinkedIn|Reddit|Amazon","architecture|deep learning","39"
"ccbfd21813","What is correlation, and how does it differ from causation?","Correlation measures the linear relationship between two variables, ranging from -1 to +1. Causation means one variable directly affects another. Correlation does not imply causation because a third confounding variable might drive both, or the relationship could be coincidental. Establishing causation typically requires randomized controlled experiments or careful causal inference methods (e.g., instrumental variables, diff-in-diff).","Statistics","easy","Samsung|Apple|NVIDIA","correlation|causation","55"
"6f23b8d9b6","What is the difference between hard margin and soft margin SVM?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Goldman Sachs|Stripe|NVIDIA","ML fundamentals|algorithms","41"
"0d63f0e15b","Explain beta distribution and its uses in ML.","Beta Distribution And Its Uses In Ml is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Meta|Capital One","probability|distributions","40"
"8e3daa1f97","How do you run experiments on new features with no baseline data?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","hard","Samsung|Netflix","experimentation|methodology","36"
"3aec98af9f","Explain continual/lifelong learning in deep learning.","Continual learning addresses catastrophic forgetting when training sequentially on multiple tasks. Approaches: (1) Regularization: EWC (elastic weight consolidation) penalizes changing important weights. (2) Replay: store/generate examples from previous tasks (experience replay, generative replay). (3) Architecture: allocate new parameters per task (progressive networks), dynamic expansion. (4) Meta-learning: learn to learn without forgetting. Evaluation: average accuracy, backward transfer, forward transfer. Active research area for deploying ML systems that learn continuously.","Deep Learning","medium","Citadel|Amazon|Stripe","continual learning|catastrophic forgetting|training","68"
"b3effd773d","Explain mixed precision training in deep learning.","Mixed precision training uses both FP16 and FP32 to speed up training while maintaining accuracy. FP16 for forward/backward pass (2x memory savings, faster compute on tensor cores), FP32 for master weights and loss scaling. Loss scaling prevents gradient underflow in FP16. Implementation: PyTorch AMP (automatic mixed precision) with GradScaler. Benefits: 2-3x speedup, half memory usage, enables larger batch sizes. BF16 (bfloat16) is an alternative with better dynamic range.","Deep Learning","medium","Spotify|Twitter|Samsung","training|performance|optimization","68"
"3c6362ade9","What is image segmentation with U-Net? Explain the architecture.","U-Net is an encoder-decoder with skip connections for pixel-wise segmentation. Encoder: downsampling with conv + pooling (captures context). Decoder: upsampling with transposed convolutions (precise localization). Skip connections concatenate encoder features to decoder at each resolution. Originally for biomedical imaging. Extensions: Attention U-Net, U-Net++, nnU-Net (self-configuring).","Computer Vision","medium","DeepMind|Salesforce|Twitter","segmentation|architecture|medical imaging","45"
"588ab90bc0","What is topic modeling?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Spotify|LinkedIn|OpenAI","NLP|language understanding","32"
"57ed3440c0","What is multicollinearity and how do you detect it?","Multicollinearity occurs when predictors are highly correlated. Effects: inflated standard errors, unstable coefficients. Detection: correlation matrix, VIF > 5-10. Solutions: remove features, PCA, regularization (Ridge), domain knowledge.","Statistics","medium","Google|Jane Street","multicollinearity|regression|diagnostics","27"
"e7a4887f3e","How would you design a customer support chatbot?","Designing a customer support chatbot requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","Databricks|Capital One","system design|production","41"
"92f7073b90","Explain maximum likelihood estimation (MLE).","MLE finds the parameter values that maximize the likelihood function L(theta|data) = P(data|theta). In practice we maximize the log-likelihood for numerical stability. MLE estimators are consistent, asymptotically normal, and asymptotically efficient (achieve the Cramer-Rao lower bound). Limitations include sensitivity to model mis-specification and potential overfitting with small samples.","Statistics","medium","Google|Apple","estimation|optimization","48"
"89edb598be","Explain the concept of pre-training vs fine-tuning.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","Pinterest|Citadel","deep learning|architecture","38"
"902994b00e","Explain Python metaclasses and give a practical ML example.","A metaclass is a class of a class -- it controls class creation. type is the default metaclass. Define custom metaclass: class Meta(type): def __new__(cls, name, bases, namespace). Practical ML example: auto-registering model classes in a registry for experiment tracking: class ModelRegistry(type): registry = {}; def __new__(cls, name, bases, ns): klass = super().__new__(cls, name, bases, ns); cls.registry[name] = klass; return klass. class BaseModel(metaclass=ModelRegistry): pass. Now all subclasses auto-register. Used in PyTorch Lightning, Keras, and MLflow for model discovery.","Python","hard","Google|ByteDance|Microsoft","metaclasses|OOP|design patterns","78"
"f720cfc4e6","Explain the CLIP model architecture and its zero-shot capabilities.","CLIP (Contrastive Language-Image Pre-training, Radford et al., 2021) jointly trains an image encoder (ViT or ResNet) and a text encoder (Transformer) to maximize cosine similarity of matching image-text pairs and minimize it for non-matching pairs (contrastive loss on 400M image-text pairs). Zero-shot classification: encode candidate labels as text ('a photo of a [class]'), compute similarity with the image embedding, and select the highest. CLIP transfers to diverse tasks without fine-tuning and enables text-guided image retrieval, generation (DALL-E), and segmentation.","Computer Vision","hard","DeepMind|Google|Walmart","multimodal|contrastive learning|zero-shot","79"
"d8dda05151","What is the No Free Lunch theorem and its practical implications?","The NFL theorem states that no single learning algorithm is universally best across all problems. Averaged over all possible data distributions, every algorithm performs equally. Practical implications: (1) domain knowledge matters -- choose algorithms suited to your problem structure, (2) always benchmark multiple approaches, (3) there is no 'best' algorithm, only best for a given dataset/task. This motivates ensemble methods and AutoML.","ML Theory","hard","Amazon|Apple|OpenAI|LinkedIn","learning theory|model selection","62"
"b83a406a90","What is weight of evidence (WOE) encoding?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","hard","DeepMind|Pinterest|Goldman Sachs","feature engineering|data preprocessing","33"
"b3d8131d8b","Write a SQL query to find customers who made purchases in 3 consecutive months.","WITH monthly AS (SELECT DISTINCT customer_id, DATE_TRUNC('month', order_date) as month FROM orders), with_lag AS (SELECT *, LAG(month, 1) OVER (PARTITION BY customer_id ORDER BY month) as prev1, LAG(month, 2) OVER (PARTITION BY customer_id ORDER BY month) as prev2 FROM monthly) SELECT DISTINCT customer_id FROM with_lag WHERE month = prev1 + INTERVAL '1 month' AND prev1 = prev2 + INTERVAL '1 month'.","SQL","medium","Apple|Airbnb|Spotify","window functions|date manipulation|analytics","61"
"feef9fd9d5","Explain Python's property decorator.","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","medium","Meta|Doordash|Spotify","Python|advanced","26"
"b768279bbc","How would you design a demand forecasting system?","Designing a demand forecasting system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","NVIDIA|Microsoft|Tesla","system design|production","41"
"a22050b3a2","How would you design a music recommendation engine?","Designing a music recommendation engine requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","LinkedIn|Walmart|Apple|Meta","system design|production","41"
"dd5dec45ca","What is an autoencoder?","Neural network trained to reconstruct input through a bottleneck. Encoder maps to latent representation, decoder reconstructs. Variants: denoising, sparse, variational (VAE), contractive. Applications: dimensionality reduction, anomaly detection, denoising.","Deep Learning","easy","Salesforce|LinkedIn|Pinterest","autoencoders|unsupervised","28"
"90402e43c9","How would you design a route optimization system for delivery?","Designing a route optimization system for delivery requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","ByteDance|Apple|NVIDIA|Databricks","system design|production","43"
"497a9e0863","What is the difference between practical and statistical significance?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Google|ByteDance|Apple","experimentation|methodology","36"
"d175bd3294","When would you choose DBSCAN over HDBSCAN and vice versa?","DBSCAN and HDBSCAN are both powerful algorithms but suited to different scenarios. DBSCAN may be preferred when its assumptions match the data characteristics, while HDBSCAN excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Microsoft|Apple|Capital One","model comparison|model selection","59"
"42f19eaabe","Explain sentiment analysis approaches.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","hard","Airbnb|LinkedIn","NLP|language understanding","32"
"bef4d8d7f2","When would you choose GMM over K-Means and vice versa?","GMM and K-Means are both powerful algorithms but suited to different scenarios. GMM may be preferred when its assumptions match the data characteristics, while K-Means excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Databricks|Airbnb|Meta","model comparison|model selection","59"
"2a9e801781","Explain efficient string processing in Python.","Understanding efficient string processing is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Uber|Apple","Python|practical skills","29"
"a0da8e8ca5","Explain depthwise separable convolutions.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Two Sigma|Amazon","deep learning|architecture","38"
"c7a0f8086a","Explain the Spearman correlation and when you would use it.","The Spearman correlation is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Google|Meta|Citadel","statistical tests|hypothesis testing","63"
"f3e6fe82cc","Explain the difference between online and offline reinforcement learning.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","Apple|Meta|Microsoft","deep learning|architecture","38"
"e745c89386","Design a SQL solution for computing retention cohorts.","WITH first_activity AS (SELECT user_id, DATE_TRUNC('month', MIN(event_date)) as cohort_month FROM events GROUP BY user_id), monthly_activity AS (SELECT DISTINCT user_id, DATE_TRUNC('month', event_date) as activity_month FROM events) SELECT f.cohort_month, EXTRACT(MONTH FROM AGE(m.activity_month, f.cohort_month)) as months_since_join, COUNT(DISTINCT m.user_id) as active_users, COUNT(DISTINCT m.user_id)::float / COUNT(DISTINCT f.user_id) as retention_rate FROM first_activity f LEFT JOIN monthly_activity m ON f.user_id = m.user_id AND m.activity_month >= f.cohort_month GROUP BY 1, 2 ORDER BY 1, 2. This produces a cohort retention matrix.","SQL","hard","Microsoft|Google|Pinterest","retention|cohort analysis|product analytics","73"
"ceba10f041","When would you choose PCA over t-SNE and vice versa?","PCA and t-SNE are both powerful algorithms but suited to different scenarios. PCA may be preferred when its assumptions match the data characteristics, while t-SNE excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Robinhood|OpenAI","model comparison|model selection","59"
"e2983eec3f","Explain data augmentation for tabular data and its importance in machine learning.","Data Augmentation For Tabular Data is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","medium","LinkedIn|Twitter","machine learning|practical ML","52"
"802ae6c637","Describe the Mistral architecture and its key innovations.","Mistral is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","NVIDIA|Meta","architecture|deep learning","39"
"afa2f7f3f2","What is the difference between a list and a tuple in Python?","Lists are mutable (can be modified after creation), defined with []. Tuples are immutable (cannot be changed), defined with (). Tuples are hashable (can be dictionary keys), slightly faster, and use less memory. Use tuples for fixed collections (coordinates, database records); use lists when you need to add/remove elements. Named tuples add field names for readability. Both support indexing, slicing, and iteration.","Python","easy","NVIDIA|Citadel","data structures|Python basics","62"
"da848f4ef0","Explain the EM algorithm and give an example.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Amazon|Google","ML fundamentals|algorithms","41"
"71998825c9","Explain the Wilcoxon signed-rank test and when you would use it.","The Wilcoxon signed-rank test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Uber|Twitter|Microsoft","statistical tests|hypothesis testing","64"
"ebd83d2ed8","Explain statistical power and how to increase it.","Statistical power = P(reject H0 | H0 is false) = 1 - beta. Higher power means better ability to detect true effects. Increase power by: larger sample, larger effect, higher alpha, lower variance, paired designs, one-tailed tests. Common target: 80% power.","Statistics","medium","Microsoft|Amazon|Capital One","power|hypothesis testing|experimental design","41"
"22ff1ed4eb","What is the difference between sync and async distributed training?","Synchronous: all workers synchronize gradients before updating. Deterministic but has straggler problem. Asynchronous: workers update independently, no barrier. Higher throughput but stale gradients. Hybrid: local SGD, gradient compression.","Deep Learning","hard","ByteDance|Huawei|Amazon","distributed training|scaling","28"
"84974fdbc1","Explain automated hyperparameter tuning and its importance in machine learning.","Automated Hyperparameter Tuning is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","hard","Capital One|OpenAI","machine learning|practical ML","50"
"595408a766","How do you create features from graph/network data?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","hard","Palantir|Uber|NVIDIA","feature engineering|data preprocessing","33"
"b259669737","Explain itertools module for data processing in Python.","Understanding itertools module for data processing is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Stripe|LinkedIn|Microsoft","Python|practical skills","31"
"e34c9d3eb3","What is Naive Bayes and when does it work well?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Robinhood|Doordash","ML fundamentals|algorithms","41"
"5c057cb2a9","Explain abstract base classes in Python.","Understanding abstract base classes is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Pinterest|Bloomberg|Microsoft","Python|practical skills","29"
"061d0b0508","What is machine translation?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","easy","Meta|Reddit","NLP|language understanding","32"
"b5cb7db2ae","Explain table partitioning in SQL.","Understanding table partitioning is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Google|Netflix|Jane Street","SQL|database","31"
"ddd2de2066","Explain the Fisher's exact test and when you would use it.","The Fisher's exact test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Goldman Sachs|Apple","statistical tests|hypothesis testing","64"
"d941e339da","What is semantic correspondence?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","Huawei|Twitter|Two Sigma","computer vision|tasks","30"
"c2ff35b2c1","Write SQL to find gaps in date sequences.","WITH date_range AS (SELECT generate_series(MIN(date), MAX(date), '1 day') as expected FROM events) SELECT d.expected FROM date_range d LEFT JOIN events e ON d.expected = e.date WHERE e.date IS NULL.","SQL","hard","Pinterest|OpenAI","gap analysis|analytics","29"
"3b02abb830","How do you handle experiments with long-term effects?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Two Sigma|Robinhood|Palantir","experimentation|methodology","36"
"4fd4d0f0ae","Explain DELETE vs TRUNCATE vs DROP.","DELETE: specific rows, logged, rollback possible, triggers fire. TRUNCATE: all rows, fast, resets identity. DROP: removes entire table. In production, prefer soft deletes (is_deleted flag).","SQL","easy","Microsoft|Uber|Amazon","DDL|DML|SQL basics","25"
"eac66474f2","Explain the z-test and when you would use it.","The z-test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","easy","Snowflake|DeepMind","statistical tests|hypothesis testing","62"
"6867fe6908","Explain the concept of pre-experiment bias detection.","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","easy","Amazon|Citadel","experimentation|methodology","36"
"da975e92ad","How would you design a voice assistant?","Designing a voice assistant requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","JPMorgan|Jane Street|DeepMind","system design|production","40"
"b49ced4e99","When and how do you use the log-rank test?","The log-rank test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","easy","Meta|Amazon","statistical tests|hypothesis testing","44"
"2be7686fff","Explain the concept of neural network sparsity.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","Doordash|Meta|Two Sigma","deep learning|architecture","38"
"40091a2349","Explain cross joins and their use cases.","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Reddit|Spotify","SQL|analytics","27"
"529f03a91d","What is dimensionality reduction and when do you need it?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Instacart|Google|JPMorgan","ML fundamentals|algorithms","41"
"d08d60398f","What is BERTScore?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","ByteDance|Amazon|Lyft","NLP|language understanding","32"
"b0aed04cee","What is spectral clustering?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Google|Salesforce|Palantir","ML fundamentals|algorithms","41"
"f7551b95e9","Explain moment generating functions.","Moment Generating Functions is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","easy","Uber|Instacart|Apple","probability|distributions","36"
"59c8415790","Explain how to detect and handle data leakage in feature engineering.","Data leakage occurs when training data contains information that would not be available at prediction time. Types: (1) Target leakage: features derived from the target (e.g., 'was_approved' in a loan approval model). (2) Train-test contamination: information from test set leaking into training (e.g., fitting scaler on full data). (3) Temporal leakage: using future information (e.g., aggregating without time cutoff). Detection: suspiciously high performance, features highly correlated with target, feature importance analysis. Prevention: strict time-based splits, pipeline with fit/transform separation, domain knowledge review, adversarial validation.","Feature Engineering","hard","Meta|Netflix|Reddit|Databricks","data leakage|validation|feature engineering","84"
"53ea8eb0d9","What is a confidence interval and how do you interpret it?","A confidence interval provides a range of plausible values for an unknown parameter. A 95% CI means if we repeated the study many times, 95% of calculated intervals would contain the true parameter. Width depends on sample size, variability, and confidence level. Narrower CIs indicate more precise estimates.","Statistics","easy","Snowflake|OpenAI|Uber","confidence intervals|inference","48"
"3ae98f43d1","Explain NDCG for ranking evaluation.","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Intel|Palantir|Amazon","ranking|evaluation","31"
"b44e634144","Explain covariance matrix.","Covariance Matrix is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Uber|Google","probability|distributions","35"
"5722d34dc1","When and how do you use the Breusch-Pagan test?","The Breusch-Pagan test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","easy","Doordash|Amazon|Stripe","statistical tests|hypothesis testing","44"
"8d1110cc28","What is Upper Confidence Bound (UCB)?","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","easy","Amazon|Uber","bandits|exploration","31"
"adb68dddff","Explain the concept of margin in classification.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","easy","NVIDIA|LinkedIn|Meta","ML fundamentals|algorithms","41"
"4dfc185874","What is automated feature engineering?","Tools: Featuretools (DFS from relational data), tsfresh (time series), AutoFeat, Feature Engine. Benefits: explores large feature spaces, finds non-obvious features. Limitations: computational cost, needs domain knowledge.","Feature Engineering","hard","Uber|Salesforce|Stripe","AutoML|feature engineering","26"
"1c2f9b5def","Explain slowly changing dimensions (SCD Type 2) in SQL.","Understanding slowly changing dimensions (SCD Type 2) is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Instacart|Snowflake","SQL|database","35"
"d1fbf70fd7","How would you design a content moderation system using ML?","Multi-stage pipeline: (1) Rule-based filters for known patterns (regex, blocklists). (2) ML classifiers: image (NSFW detection CNN), text (toxicity classifier). (3) Ensemble scoring with confidence thresholds. (4) Human review queue for borderline cases. (5) Appeal system. Infrastructure: real-time processing for new content, batch processing for existing content. Challenges: adversarial inputs, cultural context, false positives impacting user experience. Feedback loop: human labels retrain models.","System Design","medium","Citadel|NVIDIA","content moderation|classification|production","63"
"ba6d4e9aee","When would you choose Logistic Regression over SVM and vice versa?","Logistic Regression and SVM are both powerful algorithms but suited to different scenarios. Logistic Regression may be preferred when its assumptions match the data characteristics, while SVM excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","DeepMind|Lyft|Meta","model comparison|model selection","61"
"f717b5106e","What is federated learning?","Trains models across decentralized devices without sharing raw data. Process: server sends model, devices train locally, send updates, server aggregates. Challenges: non-IID data, communication, privacy attacks, poisoning. Applications: mobile keyboards, healthcare.","ML Theory","hard","Uber|Pinterest","federated learning|privacy","31"
"b79e91935d","Explain self-supervised learning and its importance in machine learning.","Self-Supervised Learning is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","medium","Tesla|Bloomberg","machine learning|practical ML","49"
"699f0db9ce","How do you create time series features?","Lags: y(t-1), y(t-7). Rolling stats: mean, std over windows. Calendar: day_of_week, month, is_holiday. Fourier features for seasonality. Differencing. External: weather, events. Avoid future leakage.","Feature Engineering","medium","Twitter|JPMorgan","time series|forecasting","24"
"500e52c80a","What are the common pitfalls in A/B testing?","Pitfalls: (1) Peeking -- checking results before predetermined end date inflates false positive rate. Solution: sequential testing (alpha spending). (2) Multiple comparisons -- testing many metrics increases false positives. Solution: Bonferroni correction, FDR control. (3) Simpson's paradox -- effect reverses when aggregated. Solution: segment analysis. (4) Network effects -- treatment affects control (e.g., social networks). Solution: cluster randomization. (5) Novelty/primacy effects -- initial behavior differs from long-term. (6) Selection bias -- non-random assignment. (7) Metric sensitivity -- wrong metric choice.","A/B Testing","medium","Doordash|Robinhood|Google","experimentation|statistical errors|methodology","80"
"fa0e698a0c","Explain the ANOVA and when you would use it.","The ANOVA is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","hard","DeepMind|Uber","statistical tests|hypothesis testing","62"
"685fb6f021","Explain visual question answering.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","easy","Amazon|Twitter|Microsoft","computer vision|tasks","30"
"424d4914a2","What are data classes?","@dataclass (Python 3.7+) auto-generates __init__, __repr__, __eq__. Features: defaults, type hints, frozen (immutable). vs namedtuple: mutable, supports inheritance. Common in ML: config objects, experiment parameters.","Python","easy","Samsung|Google|Palantir","data classes|Python","25"
"7e2f075b7c","What is image super-resolution?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","ByteDance|OpenAI|Google","computer vision|tasks","30"
"30cbd46643","Explain conditional probability.","Conditional Probability is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","easy","Amazon|Tesla|Databricks","probability|distributions","35"
"d5c7dd8016","Explain the Kruskal-Wallis test and when you would use it.","The Kruskal-Wallis test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","easy","Robinhood|Meta","statistical tests|hypothesis testing","63"
"b93e154da8","How would you design a spam detection system?","Designing a spam detection system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","Twitter|Stripe|Robinhood","system design|production","41"
"7371ea1335","Explain Python's ABC (Abstract Base Classes) module.","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","easy","Google|Robinhood|Two Sigma","Python|advanced","26"
"844b92ae18","What is feature scaling and when is it important?","Feature scaling normalizes feature ranges. Methods: (1) Standardization (z-score): mean=0, std=1. (2) Min-Max scaling: maps to [0,1]. (3) Robust scaling: uses median and IQR (outlier-resistant). (4) Log transform: reduces right skew. Important for: distance-based models (KNN, SVM, K-means), gradient-based optimization (neural networks, logistic regression). Not needed for: tree-based models (decisions based on thresholds). Always fit scaler on training data only, then transform test data.","Feature Engineering","easy","LinkedIn|Airbnb|Intel|Uber","scaling|normalization|data preprocessing","64"
"eb2fc07f3a","Explain the difference between train/val/test splits.","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Adobe|Amazon","evaluation|fundamentals","31"
"81180bfdd9","What is data preprocessing and why is it important?","This is an important concept in feature engineering that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","Feature Engineering","easy","Salesforce|ByteDance|Databricks","preprocessing|fundamentals","31"
"347a664b8c","Explain the Levene's test and when you would use it.","The Levene's test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Instacart|Citadel|NVIDIA","statistical tests|hypothesis testing","63"
"4fbd078543","Explain the elbow method for choosing k in k-means.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","LinkedIn|Apple|Capital One","ML fundamentals|algorithms","41"
"e50fe8d06e","What is dropout and why does it work?","Dropout randomly sets a fraction of neurons to zero during training. This prevents co-adaptation of neurons and acts as an implicit ensemble of sub-networks. At inference, all neurons are used with weights scaled by the keep probability. It reduces overfitting, especially in large networks. Typically p=0.5 for hidden layers and p=0.2 for input layers. Variants: spatial dropout (CNNs), DropConnect, DropBlock.","Deep Learning","easy","Two Sigma|Adobe|Apple|Bloomberg","regularization|neural networks","60"
"966e8e396f","How do you calculate percentiles in SQL?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","hard","NVIDIA|Amazon","SQL|analytics","27"
"233678cd9e","Explain the concept of attention in sequence-to-sequence models.","In seq2seq models, attention allows the decoder to focus on different parts of the input at each generation step. Mechanism: compute alignment scores between decoder hidden state and all encoder states, normalize with softmax to get attention weights, compute weighted sum (context vector). Types: additive (Bahdanau), multiplicative (Luong), scaled dot-product (Transformer). Attention solves the information bottleneck of fixed-length encoding and enables handling of long sequences.","NLP","medium","DeepMind|Tesla","attention|sequence models|translation","65"
"eb6b86c9e7","How would you design a document classification pipeline?","Designing a document classification pipeline requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","LinkedIn|OpenAI|ByteDance|NVIDIA","system design|production","41"
"f9a35f6126","How would you design a real-time fraud detection system?","Architecture: (1) Real-time feature pipeline: aggregate transaction features in sliding windows (Kafka Streams, Flink). (2) Feature store: user spending patterns, device fingerprints, location history. (3) Model: ensemble of rules, gradient-boosted trees, and a neural network. (4) Serving: sub-100ms latency, shadow mode deployment. (5) Feedback loop: labeled fraud cases retrain the model. Key features: transaction velocity, amount deviation, location anomaly, device trust score. Handle extreme class imbalance with SMOTE, focal loss, or cost-sensitive learning.","System Design","medium","Doordash|Jane Street","fraud detection|real-time systems|streaming","73"
"54fa023c41","Explain LIME and its importance in machine learning.","Lime is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","medium","Meta|Uber","machine learning|practical ML","48"
"620c1d7beb","Explain the multiple comparisons problem.","When performing multiple tests, the probability of at least one false positive increases. With m tests at alpha=0.05: P(any FP) = 1-(0.95)^m. Corrections: Bonferroni (alpha/m), Holm-Bonferroni, Benjamini-Hochberg (FDR). FDR preferred when many tests expected significant.","Statistics","hard","Google|Doordash|Tesla","multiple comparisons|FDR|hypothesis testing","35"
"0e94789876","How do you measure long-term impact of experiments?","This is an important concept in a/b testing that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","A/B Testing","medium","Samsung|Jane Street|Tesla","long-term|methodology","31"
"2f01d1c452","How does BPE (Byte Pair Encoding) tokenization work?","BPE iteratively merges the most frequent pair of adjacent tokens. Starting from individual characters, it builds a vocabulary of subword units. Algorithm: (1) Initialize vocabulary with all characters. (2) Count all adjacent token pairs. (3) Merge the most frequent pair into a new token. (4) Repeat until desired vocabulary size. Benefits: handles OOV words, compact vocabulary, language-agnostic. Used in GPT-2/3/4. Variations: byte-level BPE (GPT-2), SentencePiece (unigram model).","NLP","medium","Spotify|Two Sigma|Amazon","tokenization|text preprocessing","67"
"bbbcf91691","Explain the bootstrap method and when you would use it.","Bootstrap is a resampling technique where you draw B samples (with replacement) of size n from your data and compute the statistic of interest for each. The distribution of these B statistics approximates the sampling distribution. Use cases: estimating standard errors, constructing confidence intervals, and hypothesis testing when analytical formulas are unavailable or assumptions are violated.","Statistics","medium","Microsoft|Palantir","resampling|inference","56"
"ced561ddf4","What is feature engineering? Give examples of important techniques.","Feature engineering is the process of creating, transforming, or selecting features to improve model performance. Techniques: (1) encoding categoricals (one-hot, target encoding), (2) binning/discretization, (3) log/power transforms for skewed data, (4) interaction features, (5) polynomial features, (6) date/time decomposition, (7) aggregation features (groupby stats), (8) text features (TF-IDF, embeddings), (9) domain-specific features. Often the most impactful step in an ML pipeline.","ML Theory","medium","Snowflake|Google","feature engineering|data preprocessing","61"
"c628476ed0","Explain how Random Forests work and their advantages over single decision trees.","Random Forest is an ensemble of decision trees trained on bootstrap samples (bagging) with random feature subsets at each split. Predictions are averaged (regression) or majority-voted (classification). Advantages: reduced variance, robust to overfitting, handles high-dimensional data, provides feature importance. The randomness decorrelates trees, making the ensemble more powerful than individual trees. OOB error provides a built-in validation estimate.","ML Theory","medium","Google|Snowflake|Netflix|Tesla","ensemble methods|tree models","58"
"56ba4ff042","What is the difference between deepcopy and shallow copy?","Shallow copy (copy.copy or list.copy) creates a new object but references the same nested objects. Deep copy (copy.deepcopy) recursively copies all nested objects. Example: a = [[1,2],[3,4]]; b = copy(a) -- modifying b[0].append(5) also affects a[0]. deepcopy(a) would not. Important for: mutable default arguments, data processing pipelines, avoiding unintended side effects.","Python","easy","Walmart|Salesforce","data structures|memory|Python basics","51"
"c79fe1b110","When would you choose KNN over SVM and vice versa?","KNN and SVM are both powerful algorithms but suited to different scenarios. KNN may be preferred when its assumptions match the data characteristics, while SVM excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Meta|Citadel|NVIDIA","model comparison|model selection","59"
"35c9d32b74","Explain adapter layers for NLP.","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","ByteDance|NVIDIA","NLP|language understanding","32"
"c6d8aa06f7","What is few-shot learning in deep learning?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Snap|Huawei","deep learning|architecture","38"
"4e650649a7","Write a SQL query for a running total.","SELECT date, amount, SUM(amount) OVER (ORDER BY date ROWS UNBOUNDED PRECEDING) as running_total FROM transactions. Variants: partitioned (PARTITION BY), rolling window (ROWS N PRECEDING).","SQL","medium","Snap|Amazon|DeepMind","window functions|analytics","24"
"75aaa3a547","Write a SQL query for sessionization (grouping events into sessions with 30-minute timeout).","WITH time_diff AS (SELECT *, EXTRACT(EPOCH FROM (event_time - LAG(event_time) OVER (PARTITION BY user_id ORDER BY event_time))) / 60 as mins_since_last FROM events), session_starts AS (SELECT *, CASE WHEN mins_since_last IS NULL OR mins_since_last > 30 THEN 1 ELSE 0 END as is_new_session FROM time_diff), session_ids AS (SELECT *, SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY event_time) as session_id FROM session_starts) SELECT user_id, session_id, MIN(event_time) as session_start, MAX(event_time) as session_end, COUNT(*) as event_count FROM session_ids GROUP BY user_id, session_id.","SQL","hard","Palantir|Google|Reddit","sessionization|window functions|analytics","79"
"5bb76227df","Explain variational autoencoders (VAEs).","VAEs are generative models that learn a latent representation by maximizing the evidence lower bound (ELBO) = E_q[log p(x|z)] - KL(q(z|x) || p(z)). The encoder q(z|x) maps inputs to a latent distribution (usually Gaussian), and the decoder p(x|z) reconstructs from samples. The KL term regularizes the latent space. The reparameterization trick (z = mu + sigma * epsilon, epsilon ~ N(0,1)) enables backpropagation through sampling. Used for generation, interpolation, and semi-supervised learning.","Deep Learning","medium","Lyft|Goldman Sachs|Microsoft","generative models|latent variables|variational inference","72"
"ab6b2d6b03","Explain model interpretability and its importance in machine learning.","Model Interpretability is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","medium","Amazon|Google|Spotify","machine learning|practical ML","49"
"f1fea65ce3","Explain the concept of embedding layers in neural networks.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Doordash|Google|Reddit","deep learning|architecture","38"
"e622fe8b97","Explain the concept of self-attention vs cross-attention.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Snap|Two Sigma","deep learning|architecture","38"
"c6ee2f9e18","Explain few-shot learning approaches in computer vision.","Few-shot learning classifies new categories from very few examples. Approaches: (1) Metric learning: Siamese networks, Prototypical Networks, Matching Networks. (2) Meta-learning: MAML for fast adaptation. (3) Transfer learning: pre-train on large dataset, fine-tune on few examples. (4) Data augmentation for few-shot. (5) Foundation models (CLIP) enable zero/few-shot via text-guided classification.","Computer Vision","hard","Airbnb|Uber","few-shot learning|meta-learning|classification","50"
"c1ec5bf7b6","Derive the backpropagation algorithm for a two-layer neural network.","Consider input x, hidden h = sigma(W1*x + b1), output y_hat = W2*h + b2, loss L = (1/2)(y-y_hat)^2. Forward pass computes h and y_hat. Backward pass: dL/dy_hat = -(y-y_hat); dL/dW2 = dL/dy_hat * h^T; dL/db2 = dL/dy_hat; dL/dh = W2^T * dL/dy_hat; dL/dW1 = (dL/dh * sigma'(z1)) * x^T; dL/db1 = dL/dh * sigma'(z1). This chain rule application generalizes via computational graphs to arbitrary architectures.","ML Theory","hard","Databricks|Tesla|Google|Microsoft","neural networks|optimization|backpropagation","66"
"847dd8c9a0","How do you create features from text data for tabular models?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Netflix|Databricks|Amazon","feature engineering|data preprocessing","33"
"45c88d125a","What is a holdout group and why is it useful?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Stripe|Uber|DeepMind","experimentation|methodology","36"
"de7b208616","Explain language model perplexity.","PPL = exp(-1/N * sum(log P(w_i|context))). Lower = better prediction. Represents average branching factor. PPL=10 means choosing from 10 options. Used to compare LMs. Limitations: does not capture generation quality, depends on tokenization.","NLP","medium","Stripe|DeepMind","language models|evaluation","33"
"a2bd5b9a12","How would you design an image search engine?","Designing an image search engine requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Meta|Citadel","system design|production","41"
"d8063a0f8c","Explain data quality assessment and its importance in machine learning.","Data Quality Assessment is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","hard","Spotify|NVIDIA","machine learning|practical ML","50"
"4dbf2c80e2","What is video classification?","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","Jane Street|Citadel","computer vision|tasks","30"
"d8282bb3be","What is the difference between mean, median, and mode?","Mean is the arithmetic average (sum / count), sensitive to outliers. Median is the middle value when data is sorted, robust to outliers. Mode is the most frequent value. For symmetric distributions all three coincide. For right-skewed data: mode < median < mean. Choice depends on the data distribution and the question being asked.","Statistics","easy","Microsoft|Databricks","descriptive statistics|central tendency","54"
"bae34d5644","Explain active learning and its importance in machine learning.","Active Learning is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","hard","Amazon|Capital One|Citadel","machine learning|practical ML","49"
"9b931dc661","What is feature drift and how do you monitor it?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","medium","Bloomberg|Adobe","feature engineering|data preprocessing","33"
"8f25219835","Explain Python's GIL (Global Interpreter Lock) and its implications for ML.","The GIL is a mutex that allows only one thread to execute Python bytecode at a time in CPython. Implications: CPU-bound multithreading does not achieve true parallelism (use multiprocessing instead). However: (1) NumPy/SciPy release the GIL during C-level operations, (2) I/O-bound tasks benefit from threading, (3) PyTorch/TensorFlow use separate thread pools that bypass the GIL. For ML: data loading uses multiprocessing (DataLoader num_workers), model training uses GPU parallelism. Python 3.12+ introduces per-interpreter GIL; 3.13+ has experimental free-threaded mode.","Python","medium","Snap|LinkedIn|Doordash|Capital One","concurrency|performance|CPython","78"
"1807c259bc","How do you implement caching in Python?","@functools.lru_cache (in-memory, LRU). @functools.cache (unlimited). External: joblib.Memory (disk), Redis (distributed). For ML: cache preprocessing, features, predictions. Monitor hit rate.","Python","medium","Meta|Walmart|Lyft","caching|performance","19"
"9cac9a3767","Explain sequential testing and alpha spending.","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Airbnb|Uber|Apple","experimentation|methodology","36"
"8a8331ee51","What is 3D object detection?","Locates objects in 3D from point clouds or images. Point-based: PointNet. Voxel-based: VoxelNet. BEV: top-down projection. Multi-modal: camera+LiDAR fusion. Monocular 3D: depth from single image. Key for autonomous driving.","Computer Vision","hard","Google|OpenAI","3D detection|point clouds","29"
"61e2594028","How do you design experiments for two-sided marketplaces?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","easy","Palantir|ByteDance|Pinterest","experimentation|methodology","36"
"f725280cbb","Explain Python's context managers and the 'with' statement.","Context managers define __enter__ and __exit__ methods to manage resources. The 'with' statement ensures cleanup even if exceptions occur. Common uses: file handling, database connections, locks, temporary directories. For ML: managing GPU memory (torch.no_grad(), torch.cuda.amp.autocast()), MLflow runs. Create custom: @contextmanager def timer(): start=time.time(); yield; print(time.time()-start).","Python","medium","Meta|Stripe|NVIDIA","context managers|resource management|Python patterns","45"
"91ffd29ea5","Explain list comprehension vs generator expression.","List comprehension [x**2 for x in range(n)] creates the entire list in memory. Generator expression (x**2 for x in range(n)) produces values lazily (one at a time). Generators are memory-efficient for large sequences since they don't store all values. Use list comprehension when you need random access or the full list; use generators for iteration, large data, or when you only need values once.","Python","easy","Snap|JPMorgan|Snowflake|ByteDance","generators|memory|performance","64"
"49698c9f9c","How would you design a product categorization system?","Designing a product categorization system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Reddit|Samsung|Meta|Twitter","system design|production","41"
"bc098597b3","How would you design a data pipeline for ML?","Ingestion (Airflow/Kafka), storage (S3/Parquet/Delta), processing (Spark/dbt), feature store, validation (Great Expectations), orchestration. Principles: idempotency, schema evolution, lineage, monitoring.","System Design","medium","Meta|JPMorgan","data pipeline|MLOps","18"
"c270ec81fc","What are word embeddings?","Word embeddings are dense vector representations of words that capture semantic meaning. Words with similar meanings have similar vectors. Methods: Word2Vec (Skip-gram, CBOW), GloVe (global co-occurrence matrix factorization), FastText (subword embeddings). Properties: support arithmetic (king - man + woman is approximately queen). Limitations: static (one vector per word regardless of context). Contextual embeddings (ELMo, BERT) address this by producing different representations based on context.","NLP","easy","Uber|Salesforce|Palantir","embeddings|representation learning","64"
"2cf3f6c7e0","Explain the Transformer architecture and self-attention mechanism.","Transformers use self-attention to process sequences in parallel (unlike RNNs). Self-attention computes Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V, where Q, K, V are linear projections of input. Multi-head attention applies multiple attention functions in parallel. The architecture has encoder (bidirectional) and decoder (causal) stacks, each with attention, feed-forward layers, residual connections, and layer normalization. Transformers are the backbone of BERT, GPT, and modern NLP/vision models.","Deep Learning","medium","JPMorgan|Uber","transformers|attention|NLP","62"
"fabbd80fdb","Explain PCA and its limitations.","PCA finds orthogonal directions (principal components) that maximize variance. It projects data onto these components for dimensionality reduction. Steps: center data, compute covariance matrix, find eigenvectors/eigenvalues, project onto top-k components. Limitations: assumes linearity, sensitive to feature scaling, components may not be interpretable, captures variance not necessarily discriminative information. Alternatives: t-SNE, UMAP for visualization; LDA for classification.","ML Theory","medium","Goldman Sachs|ByteDance","dimensionality reduction|unsupervised learning","56"
"05ecd395d0","Describe the Swin Transformer architecture and its key innovations.","Swin Transformer is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Walmart|JPMorgan","architecture|deep learning","40"
"99abfa1783","When and how do you use the Welch's t-test?","The Welch's t-test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","Apple|Goldman Sachs","statistical tests|hypothesis testing","44"
"4cbd4b74de","Explain curriculum learning.","Train models easy-to-hard, mimicking human education. Benefits: faster convergence, better generalization. Approaches: self-paced learning, predefined curriculum, anti-curriculum. Applications: NLP pre-training, image classification, RL.","Deep Learning","medium","Pinterest|Bloomberg","training|learning strategies","23"
"61134f8d4f","Explain the Box-Cox transformation.","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","NVIDIA|Twitter|Meta","feature engineering|data preprocessing","33"
"a69e6d8fee","What is gradient descent?","Gradient descent is an iterative optimization algorithm that updates parameters in the direction of the negative gradient of the loss function: w = w - lr * dL/dw. Variants: batch GD (full dataset), stochastic GD (single sample), mini-batch GD (subset). Learning rate is critical -- too large causes divergence, too small causes slow convergence. Advanced optimizers: Adam, RMSprop, AdaGrad adapt the learning rate per parameter.","ML Theory","easy","Google|Jane Street|Meta|Stripe","optimization|fundamentals","65"
"d705b94617","What is constituency parsing?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","easy","Stripe|Google","NLP|language understanding","32"
"4fd19991d0","How do you handle imbalanced datasets?","Resampling (SMOTE, undersampling), class weights, focal loss, threshold tuning, balanced ensembles. Evaluation: precision-recall, AUC-PR, F1 instead of accuracy. Most important: choose the right metric.","Feature Engineering","easy","Salesforce|Apple","imbalanced data|sampling","24"
"9e9e3e80cd","Explain TabNet architecture and its importance in machine learning.","Tabnet Architecture is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Deep Learning","medium","Tesla|OpenAI","machine learning|practical ML","49"
"ac86b80fe5","How would you design a credit scoring model?","Designing a credit scoring model requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Adobe|Twitter","system design|production","41"
"e4e9ff834a","What is constitutional AI and how does it relate to AI alignment?","Constitutional AI (Anthropic, 2022) trains AI to be helpful, harmless, and honest using a 'constitution' -- a set of principles. Process: (1) Generate responses, (2) self-critique against principles, (3) revise responses. This produces training data for RLHF without human labels for harmful content. Related to alignment: ensuring AI systems act in accordance with human values. Broader alignment approaches: RLHF, debate, scalable oversight, interpretability, red-teaming.","NLP","hard","Capital One|Meta|Uber","AI safety|alignment|language models","64"
"9bffa422b5","How do you handle experiments with delayed conversions?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Amazon|Samsung","experimentation|methodology","36"
"554fc54bae","When would you choose Ridge over Lasso and vice versa?","Ridge and Lasso are both powerful algorithms but suited to different scenarios. Ridge may be preferred when its assumptions match the data characteristics, while Lasso excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Spotify|Instacart","model comparison|model selection","59"
"4be508f777","What is named entity recognition (NER)?","NER identifies and classifies named entities (persons, organizations, locations, dates, etc.) in text. Approaches: rule-based, CRF (Conditional Random Fields), BiLSTM-CRF, Transformer-based (BERT fine-tuned as token classifier). Common tag schemes: BIO (Begin, Inside, Outside), BIOES. Challenges: ambiguity (Apple the company vs. apple the fruit), nested entities, domain-specific entities. Evaluated with entity-level precision, recall, F1.","NLP","easy","Bloomberg|Netflix|OpenAI|Tesla","sequence labeling|information extraction","53"
"f558fb2d3e","When would you choose Random Forest over XGBoost and vice versa?","Random Forest and XGBoost are both powerful algorithms but suited to different scenarios. Random Forest may be preferred when its assumptions match the data characteristics, while XGBoost excels in other situations. Key factors to consider: dataset size, dimensionality, noise level, interpretability requirements, training time, and whether the relationship is linear or non-linear. Always benchmark both on your specific problem using cross-validation.","ML Theory","medium","Huawei|Reddit","model comparison|model selection","61"
"0a6990dc0a","Explain the Friedman test and when you would use it.","The Friedman test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Meta|Amazon|Apple","statistical tests|hypothesis testing","63"
"f47df59064","What is structural pattern matching (match/case) in Python 3.10+?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","medium","JPMorgan|Lyft","Python|advanced","26"
"3f6a018912","What is the Law of Large Numbers?","The Law of Large Numbers states that as the sample size increases, the sample mean converges to the population mean. The Weak LLN guarantees convergence in probability; the Strong LLN guarantees almost sure convergence. This underpins the frequentist interpretation of probability and justifies using sample statistics to estimate population parameters.","Statistics","medium","Stripe|Google","probability|convergence","50"
"17bfbb236d","When and how do you use the repeated measures ANOVA?","The repeated measures ANOVA is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","LinkedIn|Capital One","statistical tests|hypothesis testing","45"
"cbf9e5f377","What is a temp table vs CTE vs subquery? When to use each?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Google|Tesla|LinkedIn","SQL|analytics","27"
"d3c0434d57","What is beam search and how does it differ from greedy decoding?","Greedy decoding selects the highest-probability token at each step. Beam search maintains k (beam width) candidate sequences, expanding each and keeping the top-k overall. Beam search explores more of the search space and often finds higher-probability sequences. Drawbacks: higher compute, tends to produce generic/repetitive text. Alternatives: top-k sampling, nucleus (top-p) sampling, temperature scaling -- these introduce randomness for more diverse generation.","NLP","medium","Capital One|Twitter|Uber","text generation|decoding|search","61"
"fb2b9d58e4","Explain collections module (Counter, defaultdict, deque) in Python.","Understanding collections module (Counter, defaultdict, deque) is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Reddit|JPMorgan","Python|practical skills","31"
"e0a72e1576","Explain the James-Stein estimator and why it is better than MLE for high-dimensional means.","The James-Stein estimator shrinks individual sample means toward the grand mean. When estimating p >= 3 means simultaneously, it dominates (lower total MSE than) the MLE under squared error loss -- Stein's paradox. Intuitively, borrowing strength across estimates reduces variance more than the bias it introduces. This is foundational for shrinkage estimators, empirical Bayes, and regularisation in ML.","Statistics","hard","Doordash|Intel","estimation|shrinkage|high-dimensional statistics","58"
"3f71587e3d","Explain neural network quantization in deep learning.","Quantization reduces model precision: FP32 -> FP16/INT8/INT4. Types: (1) Post-training quantization (PTQ): quantize after training, fast but may lose accuracy. (2) Quantization-aware training (QAT): simulate quantization during training, better accuracy. (3) Dynamic quantization: quantize weights statically, activations dynamically. Methods: GPTQ (layer-wise), AWQ (activation-aware), GGML/GGUF (CPU-friendly). INT8 gives ~2x speedup, INT4 gives ~4x. Key for LLM deployment. Trade-off: smaller models may need calibration data.","Deep Learning","medium","Databricks|Uber|Apple","quantization|model compression|deployment","63"
"a0d5b15c9b","Explain Gaussian Processes for regression. What are the computational bottlenecks?","A GP defines a distribution over functions: f(x) ~ GP(m(x), k(x,x')). Given training data, the posterior predictive distribution is also Gaussian with closed-form mean and variance. Key: the kernel k encodes assumptions about smoothness, periodicity, etc. Bottleneck: computing the posterior requires inverting the n x n kernel matrix -- O(n^3) time, O(n^2) memory. Approximations: sparse GPs (inducing points), random features, scalable variational inference.","ML Theory","hard","Amazon|Robinhood","gaussian processes|bayesian methods|regression","63"
"757d6cca5b","What is progressive training in deep learning?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Doordash|Microsoft","deep learning|architecture","38"
"4e31c90cca","What is a recursive query and give a practical example?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","LinkedIn|Stripe|Apple","SQL|analytics","27"
"e7205aa88b","Explain style transfer.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","medium","Google|Snowflake","computer vision|tasks","30"
"4bd1fbd39c","Design a notification personalization system.","Event collection, user engagement modeling, content selection (rank by predicted engagement), send-time optimization, frequency capping (RL/bandits), channel selection, A/B testing. Key metrics: CTR, DAU, unsubscribe rate.","System Design","hard","Databricks|Tesla|NVIDIA","personalization|recommendation","26"
"57bfa736e3","Describe the Flamingo architecture and its key innovations.","Flamingo is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Meta|Bloomberg|Amazon","architecture|deep learning","39"
"18c6f39209","Explain entity embeddings for categorical features.","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Doordash|Snowflake|Huawei","feature engineering|data preprocessing","33"
"63d7e99a2e","What is the Hawthorne effect in experiments?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","JPMorgan|Apple|Pinterest","experimentation|methodology","36"
"4d3b1d77cf","Explain Bayes theorem with a real-world example.","Bayes Theorem With A Real-World Example is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Bloomberg|Salesforce|Amazon","probability|distributions","39"
"e74a1b6c95","Explain CUPED for variance reduction.","CUPED adjusts for pre-experiment behavior: Y_adj = Y - theta*X where theta=Cov(X,Y)/Var(X). Can reduce variance 50%+. Other methods: stratification, CUPAC (ML predictions), delta method for ratios.","A/B Testing","hard","Netflix|Intel|Microsoft","variance reduction|CUPED","26"
"278c042643","Explain encoder-only vs decoder-only vs encoder-decoder Transformers.","Encoder-only (BERT): bidirectional, understanding tasks. Decoder-only (GPT): causal, generation tasks. Encoder-decoder (T5): seq2seq tasks (translation, summarization). Modern trend: decoder-only at scale handles most tasks through prompting.","NLP","medium","Capital One|Airbnb","transformers|architecture","26"
"53169f8fc7","Explain Mask R-CNN.","Extends Faster R-CNN with pixel-level mask branch. Backbone (ResNet+FPN), RPN for proposals, RoI Align (bilinear interpolation), classification head, box regression, mask head. Key: decoupled mask and class prediction.","Computer Vision","hard","Jane Street|Intel","instance segmentation|detection","28"
"c0dbb792a3","How would you design a dynamic ad placement system?","Designing a dynamic ad placement system requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","medium","Apple|Amazon","system design|production","42"
"376cfcf2f3","What is monkey patching and when is it appropriate?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","hard","Apple|Meta","Python|advanced","26"
"fa0260fb86","Explain reproducibility in ML and its importance in machine learning.","Reproducibility In Ml is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","medium","Google|Walmart","machine learning|practical ML","50"
"7ce7178ca7","Explain one-hot encoding vs label encoding.","This is an important concept in feature engineering that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","Feature Engineering","medium","Samsung|OpenAI|Google","encoding|categorical","31"
"a929cbe299","Explain the difference between correlated and non-correlated subqueries.","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","easy","Google|Capital One|Stripe","SQL|analytics","27"
"e48f3914fe","Explain unit tests for ML code in Python.","Understanding unit tests for ML code is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","easy","Meta|Capital One|Robinhood","Python|practical skills","31"
"2e0bdc2ace","What is the difference between parametric and non-parametric models?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Amazon|Samsung","ML fundamentals|algorithms","41"
"5a4f4b0364","Describe the DETR architecture and its key innovations.","DETR is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Uber|Twitter","architecture|deep learning","39"
"a4feeacf07","Explain weight initialization strategies.","Xavier/Glorot: Var(w)=2/(fan_in+fan_out) for sigmoid/tanh. He/Kaiming: Var(w)=2/fan_in for ReLU. Orthogonal: preserves norm. Poor init leads to vanishing/exploding gradients. Pre-trained init (transfer learning) often best.","Deep Learning","medium","Walmart|Amazon|ByteDance","initialization|training","23"
"b5431989ac","Explain prompt engineering techniques for LLMs.","Key techniques: (1) Zero-shot: direct instruction. (2) Few-shot: provide examples in the prompt. (3) Chain-of-thought (CoT): 'Let us think step by step' -- improves reasoning. (4) Self-consistency: sample multiple CoT paths, majority vote. (5) Tree-of-thought: explore multiple reasoning branches. (6) ReAct: reasoning + action (tool use). (7) Structured output: specify format (JSON, XML). (8) System prompts: set role/context. Best practices: be specific, provide examples, break complex tasks into steps, iterate on prompts.","NLP","medium","Amazon|Meta|NVIDIA","language models|prompt engineering|practical ML","72"
"5b8867403c","What are common activation functions and their properties?","Sigmoid: output (0,1), smooth, suffers from vanishing gradients. Tanh: output (-1,1), zero-centered, also vanishing gradients. ReLU: max(0,x), computationally efficient, avoids vanishing gradients for positive inputs but can 'die' (always output 0). Leaky ReLU: allows small negative slope. GELU: smooth approximation of ReLU, used in Transformers. Swish: x*sigmoid(x), self-gated. Choice depends on architecture and task.","Deep Learning","easy","Tesla|Instacart","activation functions|neural networks","54"
"ec62837418","How would you build an ML pricing system?","Demand forecasting, price elasticity estimation (causal inference), optimization (maximize revenue subject to constraints), real-time execution, A/B testing. Approaches: GBTs for demand, LP for optimization, Thompson sampling for exploration.","System Design","hard","Apple|Bloomberg","pricing|optimization","28"
"77794dcad9","What is the BLEU score?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","medium","Jane Street|LinkedIn|Doordash","NLP|language understanding","32"
"d10bfdc562","Explain logistic regression.","Logistic regression models P(y=1|x) = sigmoid(w^T x + b) = 1/(1+exp(-(w^T x + b))). Trained by minimizing binary cross-entropy. Output is calibrated probability. Decision boundary is linear. Coefficients are log-odds ratios.","ML Theory","easy","NVIDIA|Jane Street","logistic regression|classification","31"
"7e18273026","Explain logging in ML projects in Python.","Understanding logging in ML projects is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Stripe|Goldman Sachs|Google","Python|practical skills","30"
"a40abca98c","Explain graph neural networks and its importance in machine learning.","Graph Neural Networks is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","easy","Microsoft|Salesforce","machine learning|practical ML","50"
"c3cc1e2a80","How do you handle missing values in a dataset?","Strategies: (1) Deletion: drop rows (if few missing) or columns (if >50% missing). (2) Imputation: mean/median (numerical), mode (categorical), KNN imputation, iterative (MICE). (3) Indicator: add a binary 'is_missing' feature. (4) Model-based: use algorithms that handle NaN natively (XGBoost, LightGBM). (5) Domain-specific: forward-fill for time series, 0 for counts. Choice depends on: missing mechanism (MCAR, MAR, MNAR), proportion missing, downstream model. Always analyze why data is missing first.","Feature Engineering","easy","ByteDance|Microsoft","missing data|imputation|data preprocessing","68"
"9984a88d00","Explain GANs (Generative Adversarial Networks) and their training challenges.","GANs consist of a generator G that produces fake samples and a discriminator D that distinguishes real from fake. They play a minimax game: min_G max_D E[log D(x)] + E[log(1-D(G(z)))]. Training challenges: (1) mode collapse -- G produces limited variety, (2) training instability -- oscillation/divergence, (3) vanishing gradients for G when D is too strong. Solutions: Wasserstein GAN (WGAN), spectral normalization, progressive growing, style-based architectures (StyleGAN).","Deep Learning","medium","Meta|Capital One","generative models|adversarial training","66"
"1e6af97c51","What is coreference resolution?","This NLP concept is important for building language understanding and generation systems. It covers core tasks, evaluation methods, or architectural innovations in natural language processing. Modern approaches typically leverage pre-trained Transformer models.","NLP","hard","Netflix|Snowflake|Uber","NLP|language understanding","32"
"a47cbfa8be","Explain the Central Limit Theorem.","The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's distribution, provided the population has a finite variance. This is fundamental because it justifies using normal-based confidence intervals and hypothesis tests even when the underlying data is not normal, as long as n is sufficiently large (commonly n >= 30).","Statistics","easy","Amazon|Google|JPMorgan","probability|distributions","66"
"a5fed82fc3","How do you handle environment variables in Python ML projects?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","medium","Lyft|NVIDIA","Python|advanced","26"
"ace9a40d6f","Explain Bayesian optimization for hyperparameter tuning.","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","DeepMind|Tesla","optimization|bayesian","31"
"ff710d1a9b","Explain residual connections and why they help train deep networks.","Residual connections (skip connections) add the input of a layer to its output: y = F(x) + x. This allows gradients to flow directly through the identity mapping during backpropagation, mitigating the vanishing gradient problem. The network only needs to learn the residual F(x) = H(x) - x, which is easier to optimize. Introduced in ResNet, enabling training of 100+ layer networks. Widely used in Transformers, U-Nets, and modern architectures.","Deep Learning","medium","Citadel|LinkedIn","architecture|optimization|training","70"
"c61d406f28","Explain the concept of minimum detectable effect (MDE).","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","DeepMind|Two Sigma|Apple","experimentation|methodology","36"
"0e1b664b71","How does NeRF (Neural Radiance Fields) work?","NeRF represents a 3D scene as a continuous function F: (x,y,z,theta,phi) -> (r,g,b,sigma) parameterized by an MLP. Given camera rays, volume rendering integrates color and density along each ray: C(r) = integral T(t) * sigma(r(t)) * c(r(t),d) dt, where T(t) = exp(-integral sigma(r(s))ds). Training: minimize MSE between rendered and observed pixel colors. Positional encoding (Fourier features) enables high-frequency detail. Extensions: instant-NGP (hash encoding), Mip-NeRF, dynamic NeRF.","Computer Vision","hard","Amazon|Instacart","3D vision|neural rendering|volumetric rendering","66"
"a7defb5521","What is stemming vs lemmatization?","Stemming: rule-based suffix removal (running->run, studies->studi). Fast but may produce non-words. Lemmatization: dictionary-based, returns proper lemma (better->good). Slower but accurate. With subword tokenization, neither is typically needed.","NLP","easy","Stripe|Airbnb|Microsoft","text preprocessing|NLP","27"
"a6988683a8","Explain k-means clustering and its limitations.","K-means partitions n data points into k clusters by iteratively assigning points to the nearest centroid and updating centroids as cluster means. Converges to a local minimum of within-cluster sum of squares. Limitations: must specify k, assumes spherical/equal-size clusters, sensitive to initialization (use k-means++), sensitive to outliers, only finds convex clusters. Alternatives: DBSCAN, hierarchical clustering, GMM.","ML Theory","medium","Netflix|Meta","clustering|unsupervised learning","56"
"4fdd446c82","Describe the EfficientNet architecture and its key innovations.","EfficientNet is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Google|Walmart|Stripe","architecture|deep learning","39"
"df0213673e","How do you analyze experiments with non-normal metrics?","This experimentation concept is critical for running rigorous A/B tests at scale. Understanding it helps avoid common pitfalls and ensures valid causal conclusions from experiments. It requires knowledge of statistics, experimental design, and practical engineering considerations.","A/B Testing","medium","Samsung|JPMorgan|Databricks","experimentation|methodology","36"
"17fabe0241","What is the silhouette score?","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","NVIDIA|Uber|Walmart","ML fundamentals|algorithms","41"
"b3f17a0008","What are window functions in SQL? Give examples.","Window functions perform calculations across a set of rows related to the current row without collapsing them (unlike GROUP BY). Syntax: function() OVER (PARTITION BY col ORDER BY col ROWS/RANGE frame). Examples: ROW_NUMBER(), RANK(), DENSE_RANK(), LAG(), LEAD(), SUM() OVER, running averages, cumulative sums. Use cases: ranking within groups, running totals, comparing to previous/next rows, percentiles. Window functions execute after WHERE, GROUP BY, and HAVING.","SQL","easy","Twitter|Bloomberg|Google|OpenAI","window functions|analytics","64"
"139089253a","Explain joint and marginal distributions.","Joint And Marginal Distributions is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Intel|Snap|Meta","probability|distributions","37"
"ea2023a4ed","What is the Global Interpreter Lock workarounds for ML?","This Python concept is relevant for ML engineers writing production code. It demonstrates understanding of Python's advanced features, design patterns, and best practices for maintainable software.","Python","easy","Airbnb|OpenAI","Python|advanced","26"
"05cc888a99","What is knowledge distillation?","Knowledge distillation trains a smaller 'student' model to mimic a larger 'teacher' model. The student learns from the teacher's soft predictions (logits/probabilities at a temperature T) in addition to hard labels. The softened outputs carry 'dark knowledge' about class similarities. Loss = alpha * CE(y, student_pred) + (1-alpha) * KL(teacher_soft, student_soft). Benefits: model compression, faster inference, deployment on edge devices. Used in DistilBERT, TinyBERT.","Deep Learning","medium","Instacart|NVIDIA|Microsoft","model compression|deployment|training","64"
"11ef5570f4","Explain exponential distribution and memoryless property.","Exponential Distribution And Memoryless Property is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Snap|Databricks|Samsung","probability|distributions","38"
"5f696ce776","Describe the DenseNet architecture and its key innovations.","DenseNet is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Bloomberg|Databricks","architecture|deep learning","39"
"b859344330","Explain SHAP values and its importance in machine learning.","Shap Values is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","easy","Jane Street|NVIDIA|Stripe","machine learning|practical ML","49"
"f5704e51d1","What is the Kolmogorov-Smirnov test?","The KS test is a non-parametric test that compares a sample distribution with a reference distribution (one-sample) or two sample distributions (two-sample). The test statistic is the maximum absolute difference between the two CDFs. It tests whether the distributions are the same. Advantages: distribution-free. Limitations: less sensitive to differences in the tails and requires continuous distributions.","Statistics","medium","Twitter|Apple|OpenAI","statistical tests|distributions","56"
"e162ca6bfe","Explain query optimization with multiple JOINs in SQL.","Understanding query optimization with multiple JOINs is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Amazon|Adobe|Samsung","SQL|database","34"
"f8ceba060c","How would you build a system to serve LLM inference at scale?","Architecture: (1) Model optimization: quantization (GPTQ, AWQ), KV-cache optimization, speculative decoding, continuous batching. (2) Serving: vLLM, TGI, or TensorRT-LLM for efficient GPU utilization. (3) Infrastructure: GPU clusters with NVLink, load balancing across replicas. (4) API layer: rate limiting, authentication, streaming responses (SSE). (5) Caching: semantic cache for similar queries. (6) Monitoring: tokens/second, time-to-first-token, GPU utilization. Cost optimization: spot instances, model routing (small model for simple queries, large model for complex ones). Scaling: autoscaling based on queue depth.","System Design","hard","Citadel|LinkedIn|Amazon|Microsoft","LLM|inference|serving|infrastructure","77"
"789a1e9010","What is mixup training?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","Adobe|Pinterest","deep learning|architecture","38"
"ab3df402b5","Describe the LLaMA architecture and its key innovations.","LLaMA is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","Doordash|Samsung","architecture|deep learning","39"
"4bf509901a","How would you design a real-time feature engineering pipeline?","Architecture: (1) Event ingestion: Kafka/Kinesis for real-time events. (2) Stream processing: Flink/Spark Streaming for feature computation. (3) Feature store: online store (Redis/DynamoDB) for low-latency serving, offline store (S3/BigQuery) for training. (4) Batch features: daily/hourly aggregations via Airflow. (5) Feature serving API: <10ms latency. (6) Consistency: ensure training features match serving features. Tools: Feast, Tecton, Databricks Feature Store.","System Design","hard","Goldman Sachs|Meta|DeepMind","feature engineering|streaming|infrastructure","57"
"77a9e143db","When and how do you use the likelihood ratio test?","The likelihood ratio test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","hard","JPMorgan|Spotify|Doordash","statistical tests|hypothesis testing","45"
"f2fb8d91d4","Explain model registries and their role in MLOps.","This is an important concept in system design that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","System Design","medium","Capital One|DeepMind|LinkedIn","MLOps|deployment","31"
"c28768ba01","Explain the Bartlett's test and when you would use it.","The Bartlett's test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","DeepMind|Microsoft|Amazon","statistical tests|hypothesis testing","63"
"f71c2dafc4","Explain dataclasses vs pydantic in Python.","Understanding dataclasses vs pydantic is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","ByteDance|Palantir|JPMorgan","Python|practical skills","29"
"2a17236a3b","Explain Thompson Sampling.","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","Meta|Bloomberg|Huawei","bandits|optimization","31"
"4815af6dfc","Explain clustered vs non-clustered indexes in SQL.","Understanding clustered vs non-clustered indexes is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","hard","Google|Huawei","SQL|database","33"
"933f642356","Explain partial dependence plots and its importance in machine learning.","Partial Dependence Plots is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","Feature Engineering","hard","Microsoft|Capital One|NVIDIA","machine learning|practical ML","50"
"c1edc072de","Explain the attention mechanism mathematically and derive its gradient.","Given Q, K, V in R^{n x d}: A = softmax(QK^T/sqrt(d)) V. Let S = QK^T/sqrt(d), P = softmax(S) (row-wise). Forward: A = PV. Backward: dL/dV = P^T dL/dA; dL/dP = dL/dA V^T; dL/dS_ij = P_ij(dL/dP_ij - sum_k P_ik dL/dP_ik) (softmax Jacobian); dL/dQ = dL/dS K/sqrt(d); dL/dK = dL/dS^T Q/sqrt(d). The sqrt(d) scaling prevents softmax saturation. Flash Attention optimizes this with tiling to reduce memory I/O.","Deep Learning","hard","Goldman Sachs|Amazon","attention|transformers|optimization","66"
"7c4f3ce7be","Explain DBSCAN clustering and its advantages.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","medium","Meta|Palantir|Netflix","ML fundamentals|algorithms","41"
"b4e204f135","What is transfer learning and when should you use it?","Transfer learning uses a model pre-trained on a large dataset as a starting point for a new task. Approaches: (1) feature extraction -- freeze pre-trained layers, train a new head, (2) fine-tuning -- unfreeze some/all layers and train with a small learning rate. Use when: you have limited labeled data, your task is related to the pre-training task, or you need faster convergence. Examples: ImageNet-pretrained CNNs, BERT/GPT for NLP, foundation models.","Deep Learning","medium","Walmart|Two Sigma|Bloomberg","transfer learning|fine-tuning|practical ML","71"
"4131e90d1e","Explain time series cross-validation.","This is an important concept in ml theory that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","ML Theory","medium","LinkedIn|ByteDance","time series|validation","31"
"e282845ded","Explain feature selection methods: filter, wrapper, and embedded.","Filter: rank features independently. Metrics: correlation, mutual information, chi-squared, ANOVA, variance threshold. Fast but ignores interactions. Wrapper: evaluate subsets with a model. Techniques: forward selection, backward elimination, RFE. More accurate but expensive. Embedded: selection during training. Examples: L1 (Lasso), tree-based importance, attention weights. Best practice: start with filters, then use embedded methods.","Feature Engineering","medium","NVIDIA|Apple|Uber","feature selection|model selection|dimensionality reduction","52"
"334c7fe9c4","Explain recursive feature elimination (RFE).","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Netflix|Twitter|Amazon","feature engineering|data preprocessing","33"
"df185a4625","What is the difference between generative and discriminative models?","Discriminative: learn P(y|x) directly (logistic regression, SVM, neural nets). Generative: learn P(x,y)=P(x|y)P(y) (Naive Bayes, HMMs, GANs, VAEs). Discriminative typically better for classification; generative can generate samples.","ML Theory","easy","Uber|Twitter|Tesla","model types|probability","26"
"a4498a7544","Explain the concept of expected loss in Bayesian testing.","This is an important concept in a/b testing that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","A/B Testing","easy","Two Sigma|ByteDance|Microsoft","bayesian|decision making","31"
"0b2fa0b522","What is causal inference and how does it differ from prediction?","Causal inference estimates the effect of interventions, while prediction estimates Y given observed X. Causal requires: randomization (RCTs) or observational methods (IV, diff-in-diff, regression discontinuity, propensity score). Frameworks: Rubin Causal Model, Pearl do-calculus.","Statistics","hard","Two Sigma|Pinterest|OpenAI","causal inference|counterfactuals","33"
"e4679fae37","Write a SQL query to detect fraudulent transactions (amount > 3 std devs from user mean).","WITH user_stats AS (SELECT user_id, AVG(amount) as avg_amt, STDDEV(amount) as std_amt FROM transactions GROUP BY user_id), flagged AS (SELECT t.*, u.avg_amt, u.std_amt, ABS(t.amount - u.avg_amt) / NULLIF(u.std_amt, 0) as z_score FROM transactions t JOIN user_stats u ON t.user_id = u.user_id) SELECT * FROM flagged WHERE z_score > 3 ORDER BY z_score DESC.","SQL","hard","Google|Microsoft|Jane Street","anomaly detection|analytics|fraud","52"
"37a0479e1d","What is the theory behind contrastive learning (e.g., SimCLR, MoCo)?","Contrastive learning learns representations by pulling positive pairs (augmented views of the same sample) together and pushing negative pairs apart in embedding space. SimCLR loss (NT-Xent): -log[exp(sim(z_i,z_j)/tau) / sum_k exp(sim(z_i,z_k)/tau)]. Key ingredients: strong data augmentation, large batch size (or momentum encoder in MoCo for negative samples), projection head, temperature scaling. Theoretical connections to mutual information maximization and spectral methods.","Deep Learning","hard","Palantir|Google","self-supervised learning|representation learning","59"
"7f4a4a1e6e","What are the key differences between BERT and GPT?","BERT: encoder-only, bidirectional attention, pre-trained with masked language modeling (MLM) and next sentence prediction. Best for understanding tasks (classification, NER, QA). GPT: decoder-only, causal (left-to-right) attention, pre-trained with next-token prediction. Best for generation tasks. BERT sees full context; GPT generates autoregressively. GPT-3/4 show emergent few-shot learning via in-context learning. Modern trend: scaling decoder-only models.","Deep Learning","medium","Jane Street|ByteDance|Uber|Meta","transformers|language models|NLP","54"
"e552e77902","Explain Poisson distribution applications.","Poisson Distribution Applications is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","medium","Google|OpenAI","probability|distributions","36"
"e0fb84ae37","What metrics for a search engine A/B test?","Primary: CTR, time to first click, successful session rate. Guardrail: queries per session, pogo-sticking, zero-result rate. Revenue: ads revenue per search. Segment by query type.","A/B Testing","medium","Amazon|Tesla|Twitter","metrics|search|experimentation","25"
"8be6c96269","Explain technical debt in ML systems and its importance in machine learning.","Technical Debt In Ml Systems is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","medium","Airbnb|JPMorgan|Apple","machine learning|practical ML","52"
"0ad95050b8","What is overfitting and how do you prevent it?","Overfitting occurs when a model learns noise in the training data rather than the underlying pattern, leading to poor generalization. Prevention strategies: (1) more training data, (2) regularization (L1/L2), (3) cross-validation for model selection, (4) early stopping, (5) dropout (neural networks), (6) ensemble methods, (7) feature selection/dimensionality reduction, (8) data augmentation.","ML Theory","easy","Jane Street|Bloomberg|Intel|Meta","regularization|model selection","51"
"e601561e38","Explain OLTP vs OLAP databases in SQL.","Understanding OLTP vs OLAP databases is important for data engineering and analytics. It helps in designing efficient schemas, writing performant queries, and making appropriate architectural decisions. Requires knowledge of database internals and optimization.","SQL","medium","Google|Huawei|Meta","SQL|database","33"
"6acffd756a","How do you handle time zones in SQL?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","ByteDance|Meta","SQL|analytics","27"
"8a7c4c9ff5","What is semantic search and how is it implemented?","Semantic search retrieves documents based on meaning, not just keyword matching. Implementation: (1) Encode documents and queries as dense vectors using a bi-encoder (BERT, sentence-transformers). (2) Index document vectors with ANN library: FAISS, Annoy, ScaNN, Pinecone, Weaviate. (3) At query time, encode query and find nearest document vectors. Training: contrastive learning on query-document pairs. Hybrid: combine BM25 (lexical) with dense retrieval. Reranking with cross-encoder for top-k candidates improves precision.","NLP","medium","Apple|Huawei|Uber","search|embeddings|information retrieval","69"
"87952d0d04","Describe the RWKV architecture and its key innovations.","RWKV is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","hard","Instacart|Capital One|Snowflake","architecture|deep learning","39"
"c6c2594c79","How do you create features from geospatial data?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","medium","Airbnb|LinkedIn|OpenAI","feature engineering|data preprocessing","33"
"c7c74eb3e3","What is online vs batch learning?","Batch: trains on entire dataset. Online: updates model incrementally per new data point. Online needed for: streaming data, data too large for memory, distribution shifts. Algorithms: SGD, perceptron. Challenges: catastrophic forgetting.","ML Theory","medium","Microsoft|DeepMind|Amazon","learning paradigms|streaming","31"
"73a581e45d","What is the role of temperature in softmax?","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","hard","Netflix|JPMorgan|Robinhood","deep learning|architecture","38"
"97aac5a2f1","How would you design a real-time translation service?","Designing a real-time translation service requires careful ML pipeline design, data requirements, model selection, serving infrastructure, and evaluation. Key components: data collection, feature engineering, model training, A/B testing, monitoring for drift, and continuous improvement. Design for scalability, low latency, and reliability.","System Design","hard","DeepMind|Capital One|Robinhood","system design|production","41"
"3d2255db53","Explain multi-modal learning.","This computer vision task involves processing and understanding visual information. Modern approaches use deep CNNs, Vision Transformers, or multi-modal architectures. Key considerations include data requirements, evaluation metrics, and computational costs.","Computer Vision","hard","DeepMind|Two Sigma","computer vision|tasks","30"
"4913c25457","Explain cumulative distribution functions.","Cumulative Distribution Functions is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","easy","Meta|Capital One","probability|distributions","36"
"67ddfd229f","Explain semi-supervised learning and its importance in machine learning.","Semi-Supervised Learning is an important concept in modern ML. It addresses key challenges in building robust systems. Understanding it is essential for production ML, as it directly impacts performance, reliability, and maintainability. Key considerations include when to apply it, trade-offs involved, and best practices established by the ML community.","ML Theory","easy","NVIDIA|ByteDance|Tesla","machine learning|practical ML","49"
"e94b5a172e","Explain pathlib for file operations in Python.","Understanding pathlib for file operations is practical for Python ML developers. It helps write cleaner, maintainable, efficient code. Important for production ML where code quality impacts reliability and development speed.","Python","medium","Databricks|Salesforce|Amazon","Python|practical skills","30"
"be25ef0ece","Explain learning rate scheduling in deep learning.","Learning rate scheduling adjusts the learning rate during training. Common strategies: step decay, cosine annealing, warmup + linear decay, cyclic learning rates, one-cycle policy. Warmup gradually increases LR from a small value to avoid instability early in training. Cosine annealing smoothly decreases LR. OneCycleLR (Smith, 2019) uses a single cycle of increasing then decreasing LR, often achieving super-convergence. Choice depends on the architecture and task.","Deep Learning","medium","OpenAI|Goldman Sachs","training|optimization","65"
"f4cc33d50d","Explain the binomial test and when you would use it.","The binomial test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","hard","Lyft|Reddit","statistical tests|hypothesis testing","63"
"2a523e1b7b","Explain gradient accumulation in deep learning.","Gradient accumulation simulates larger batch sizes by accumulating gradients over multiple forward/backward passes before updating weights. Effective batch size = micro_batch * accumulation_steps. Useful when GPU memory limits batch size. Implementation: zero gradients, loop over accumulation steps, divide loss by steps, optimizer step. Considerations: batch norm statistics may differ, learning rate should correspond to effective batch size.","Deep Learning","hard","Apple|Amazon","training|optimization|memory","57"
"eeecfc857c","Explain the Gini impurity measure.","This is a core ML concept tested in data science interviews. Understanding it demonstrates knowledge of ML fundamentals, model selection criteria, and practical application skills. Key aspects include algorithmic details, computational complexity, assumptions, and when to use this approach versus alternatives.","ML Theory","hard","Airbnb|Spotify|Walmart","ML fundamentals|algorithms","41"
"529849350d","What are *args and **kwargs?","*args collects positional arguments into a tuple: def f(*args) allows f(1,2,3). **kwargs collects keyword arguments into a dictionary: def f(**kwargs) allows f(a=1,b=2). Used for flexible function signatures, decorators, and passing arguments to other functions. Order: positional, *args, keyword, **kwargs. Example: def wrapper(*args, **kwargs): return original_func(*args, **kwargs).","Python","easy","OpenAI|Meta|ByteDance","functions|Python basics","46"
"ca8045b9b3","Explain positional encoding in Transformers.","This deep learning concept is frequently discussed in ML engineering interviews. It relates to neural network architecture design, training optimization, or inference efficiency. Understanding it helps build more effective models and demonstrate knowledge of modern deep learning practices.","Deep Learning","medium","Stripe|Pinterest","deep learning|architecture","38"
"fb562acace","Design a large-scale search ranking system (like Google Search).","Multi-stage architecture: (1) Query understanding: intent classification, query expansion, spell correction. (2) Retrieval: inverted index (BM25), approximate nearest neighbors (HNSW) for semantic search using query/document embeddings. (3) Ranking: L1 (fast, simple features, hundreds of candidates), L2 (complex model -- cross-encoder BERT or LambdaMART, dozens of candidates). (4) Blending: mix organic results, ads, knowledge panels. Features: textual relevance, click-through rate, freshness, authority (PageRank). Training: learning-to-rank with human relevance labels. Serving at scale: sharding, caching, streaming aggregation.","System Design","hard","Amazon|Meta","search|ranking|information retrieval","75"
"2579379686","What is a lateral join?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","Capital One|Reddit|Samsung","SQL|analytics","27"
"1bd4f3051b","When and how do you use the Durbin-Watson test?","The Durbin-Watson test is used to test specific statistical hypotheses. Key requirements include understanding the null hypothesis, assumptions (normality, independence, equal variance), and interpreting the test statistic and p-value. Always verify assumptions and consider effect sizes alongside significance. Alternatives exist when assumptions are violated.","Statistics","medium","Adobe|Databricks|Meta","statistical tests|hypothesis testing","44"
"51fcf80044","What is A/B testing infrastructure?","Components: experiment config, hash-based user assignment, exposure logging, metric computation, statistical analysis, dashboard, guardrail metrics. Tools: Optimizely, LaunchDarkly, internal platforms.","System Design","medium","Tesla|Adobe|Meta","experimentation|infrastructure","20"
"6bb399166c","How do you write efficient date range queries?","This SQL concept is commonly tested in data science and analytics interviews. It requires understanding query logic, performance implications, and practical use cases in data analysis workflows.","SQL","medium","Uber|Intel|Palantir","SQL|analytics","27"
"87a142c45b","Explain the Mann-Whitney U test and when you would use it.","The Mann-Whitney U test is a statistical test commonly used in data analysis. It is appropriate when testing specific hypotheses about your data distribution or relationships between variables. Key considerations include assumptions about data (normality, independence, sample size), the null and alternative hypotheses, and interpretation of the test statistic and p-value. Always verify assumptions before applying the test and consider effect sizes alongside p-values.","Statistics","medium","Intel|OpenAI","statistical tests|hypothesis testing","64"
"e62886faca","What is regression to the mean in experiments?","This is an important concept in a/b testing that is frequently discussed in technical interviews. Understanding it demonstrates depth in ML theory and practical application skills valued by top tech companies.","A/B Testing","medium","Meta|Bloomberg|Apple","statistical artifacts|methodology","31"
"ecc7c2fa1e","What is the bias-variance tradeoff?","The bias-variance tradeoff describes the tension between a model's ability to fit the training data (low bias) and its ability to generalize (low variance). Total error = Bias^2 + Variance + Irreducible noise. Simple models have high bias / low variance (underfitting); complex models have low bias / high variance (overfitting). The optimal model minimizes total error.","ML Theory","easy","Twitter|LinkedIn|Netflix|Google","model selection|generalization","57"
"eab65c86c2","What is the curse of dimensionality?","As features increase, feature space grows exponentially. Data becomes sparse, distances become less meaningful, models need exponentially more data, overfitting risk increases. Solutions: PCA, feature selection, regularization, UMAP.","ML Theory","easy","Spotify|Walmart|Apple","dimensionality|fundamentals","28"
"6606e69164","Describe the architecture and training of mixture-of-experts (MoE) language models.","MoE replaces dense feed-forward layers with multiple expert networks and a gating function that routes each token to top-k experts. Architecture: gate(x) selects k experts (typically k=1 or 2), output = sum of selected expert outputs weighted by gate values. Training: auxiliary load-balancing loss ensures experts are used evenly. Benefits: scale model parameters without proportional compute increase (only active experts are computed). Examples: Switch Transformer, GLaM, Mixtral. Challenges: communication costs, expert collapse, training instability.","NLP","hard","Netflix|Amazon","language models|architecture|scaling","74"
"cce7051d1e","How do you handle categorical variables?","Encoding methods: (1) One-hot: binary column per category (use for low cardinality). (2) Label/ordinal encoding: integer mapping (preserves order for ordinal features). (3) Target encoding: replace category with mean target value (regularize to avoid overfitting). (4) Frequency encoding: replace with category frequency. (5) Embedding: learned dense vectors (deep learning). (6) Binary encoding: binary representation of label encoding. Tree-based models handle ordinal encoding well; linear models typically need one-hot. Watch out for high cardinality and unseen categories.","Feature Engineering","easy","Twitter|OpenAI|DeepMind","categorical encoding|data preprocessing","76"
"8fc8b4e7b1","Describe the Segment Anything (SAM) architecture and its key innovations.","Segment Anything (SAM) is a notable deep learning architecture with important innovations. It addresses limitations of prior approaches and has been widely adopted. Key choices include its feature extraction approach, computational efficiency, and scalability. Understanding it is valuable for state-of-the-art systems.","Deep Learning","medium","DeepMind|Stripe","architecture|deep learning","41"
"c6c9ef8730","Explain multivariate normal distribution.","Multivariate Normal Distribution is a fundamental concept in probability and statistics with direct applications in machine learning. Understanding it enables better model design, proper uncertainty quantification, and correct interpretation of statistical results in data science workflows.","Statistics","easy","Meta|Google|Microsoft","probability|distributions","36"
"b1ed2ceb87","What is binning/discretization and when should you use it?","This feature engineering technique is essential for building high-quality ML models. It transforms raw data into meaningful representations that improve model performance. Key considerations include data types, model compatibility, and avoiding data leakage.","Feature Engineering","easy","Google|Amazon","feature engineering|data preprocessing","33"
"6f27734eee","Explain Python's memory management and garbage collection.","Python uses reference counting as the primary GC mechanism: each object tracks how many references point to it; when the count reaches 0, memory is freed. Cyclic references (A -> B -> A) cannot be freed by reference counting alone, so Python has a generational garbage collector that periodically detects and collects cycles. Three generations (0, 1, 2) with decreasing collection frequency. gc module allows manual control. For ML: use del and gc.collect() to free GPU memory; be cautious with circular references in data pipelines.","Python","medium","Databricks|Uber|Palantir|LinkedIn","memory management|garbage collection|performance","85"
